{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a84784",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Machine learning is a method of data analysis that automates analytical model building. Using algorithms that iteratively learn from data, machine learning allows computers to find hidden insights without being explicitly programmed. ^-^\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52bb94",
   "metadata": {},
   "source": [
    "# B√ÄI T·∫¨P 1: TI·ªÄN X·ª¨ L√ù VƒÇN B·∫¢N (B·∫ÆT BU·ªòC)\n",
    "\n",
    "Vi·∫øt code ƒë·ªÉ th·ª±c hi·ªán ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n tr√™n ƒëo·∫°n vƒÉn b·∫£n tr√™n, bao g·ªìm c√°c b∆∞·ªõc:\n",
    "\n",
    "- Chuy·ªÉn t·∫•t c·∫£ ch·ªØ v·ªÅ lowercase.\n",
    "- Tokenization: Chia vƒÉn b·∫£n th√†nh c√°c t·ª´ ri√™ng l·∫ª.\n",
    "- Lo·∫°i b·ªè stopwords.\n",
    "- Lemmatization ƒë·ªÉ ƒë∆∞a t·ª´ v·ªÅ d·∫°ng nguy√™n th·ªÉ.\n",
    "- Lo·∫°i b·ªè d·∫•u c√¢u v√† k√Ω t·ª± ƒë·∫∑c bi·ªát."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42c89f",
   "metadata": {},
   "source": [
    "## Chuy·ªÉn t·∫•t c·∫£ ch·ªØ v·ªÅ lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fccab52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning is a method of data analysis that automates analytical model building. using algorithms that iteratively learn from data, machine learning allows computers to find hidden insights without being explicitly programmed. ^-^\n"
     ]
    }
   ],
   "source": [
    "# chuy·ªÉn t·∫•t c·∫£ v·ªÅ lowercase\n",
    "Lower_text = text.lower()\n",
    "print(Lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc30a01",
   "metadata": {},
   "source": [
    "## Tokenization: Chia vƒÉn b·∫£n th√†nh c√°c t·ª´ ri√™ng l·∫ª.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5e27373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'is', 'a', 'method', 'of', 'data', 'analysis', 'that', 'automates', 'analytical', 'model', 'building', '.', 'using', 'algorithms', 'that', 'iteratively', 'learn', 'from', 'data', ',', 'machine', 'learning', 'allows', 'computers', 'to', 'find', 'hidden', 'insights', 'without', 'being', 'explicitly', 'programmed', '.', '^-^']\n"
     ]
    }
   ],
   "source": [
    "# chia vƒÉn b·∫£n th√†nh c√°c t·ª´ ri√™ng l·∫ª\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tokenization\n",
    "tokenization_text = word_tokenize(Lower_text)\n",
    "\n",
    "print(tokenization_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30684f7",
   "metadata": {},
   "source": [
    "## Lo·∫°i b·ªè stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fdbd9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "--------------------\n",
      "['machine', 'learning', 'is', 'a', 'method', 'of', 'data', 'analysis', 'that', 'automates', 'analytical', 'model', 'building', '.', 'using', 'algorithms', 'that', 'iteratively', 'learn', 'from', 'data', ',', 'machine', 'learning', 'allows', 'computers', 'to', 'find', 'hidden', 'insights', 'without', 'being', 'explicitly', 'programmed', '.', '^-^']\n",
      "['machine', 'learning', 'method', 'data', 'analysis', 'automates', 'analytical', 'model', 'building', '.', 'using', 'algorithms', 'iteratively', 'learn', 'data', ',', 'machine', 'learning', 'allows', 'computers', 'find', 'hidden', 'insights', 'without', 'explicitly', 'programmed', '.', '^-^']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport nltk\\n# nltk.download(\\'stopwords\\')  # B·ªè comment d√≤ng n√†y n·∫øu b·∫°n ch∆∞a t·∫£i stopwords\\n\\n# L·∫•y danh s√°ch stopwords ti·∫øng Anh\\nenglish_stopwords = nltk.corpus.stopwords.words(\"english\")\\n\\n# H√†m lo·∫°i b·ªè stopwords\\ndef remove_stopwords(tokens):\\n    return [token for token in tokens if token not in english_stopwords]\\n\\n# V√≠ d·ª• s·ª≠ d·ª•ng\\ntokens = [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\\nfiltered = remove_stopwords(tokens)\\nprint(\"Filtered tokens:\", filtered)\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE 4.3\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "english_stopwords = nltk.corpus.stopwords.words(\"english\") # nltk.corpus.stopwords: Truy c·∫≠p t·∫≠p h·ª£p t·ª´ d·ª´ng c√≥ s·∫µn trong NLTK.\n",
    "                                                           # .words(\"english\"): L·∫•y danh s√°ch stopwords c·ªßa ti·∫øng Anh.\n",
    "print(len(english_stopwords))\n",
    "print(english_stopwords)\n",
    "print(\"-\"*20)\n",
    "\n",
    "def remove_stopwords(tokens, language = \"english\"):\n",
    "    stopword_list = nltk.corpus.stopwords.words(language)\n",
    "    filter_tokens = [token for token in tokens if token not in stopword_list] #Duy·ªát qua t·ª´ng token trong tokens\n",
    "                                                                              # Ch·ªâ gi·ªØ l·∫°i nh·ªØng t·ª´ kh√¥ng n·∫±m trong stopword_list\n",
    "    return filter_tokens\n",
    "\n",
    "print(tokenization_text)\n",
    "#nltk.download('stopwords')\n",
    "stopwords_text = remove_stopwords(tokenization_text)\n",
    "print(stopwords_text)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import nltk\n",
    "# nltk.download('stopwords')  # B·ªè comment d√≤ng n√†y n·∫øu b·∫°n ch∆∞a t·∫£i stopwords\n",
    "\n",
    "# L·∫•y danh s√°ch stopwords ti·∫øng Anh\n",
    "english_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# H√†m lo·∫°i b·ªè stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in english_stopwords]\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng\n",
    "tokens = [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\n",
    "filtered = remove_stopwords(tokens)\n",
    "print(\"Filtered tokens:\", filtered)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda12799",
   "metadata": {},
   "source": [
    "## Lemmatization ƒë·ªÉ ƒë∆∞a t·ª´ v·ªÅ d·∫°ng nguy√™n th·ªÉ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c0b869a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization:\n",
      "machine -> machine\n",
      "learning -> learning\n",
      "method -> method\n",
      "data -> data\n",
      "analysis -> analysis\n",
      "automates -> automates\n",
      "analytical -> analytical\n",
      "model -> model\n",
      "building -> building\n",
      "using -> using\n",
      "algorithms -> algorithm\n",
      "iteratively -> iteratively\n",
      "learn -> learn\n",
      "data -> data\n",
      "machine -> machine\n",
      "learning -> learning\n",
      "allows -> allows\n",
      "computers -> computer\n",
      "find -> find\n",
      "hidden -> hidden\n",
      "insights -> insight\n",
      "without -> without\n",
      "explicitly -> explicitly\n",
      "programmed -> programmed\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization ƒë·ªÉ ƒë∆∞a t·ª´ v·ªÅ d·∫°ng nguy√™n th·ªÉ.\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "# nltk.download('wordnet')  # N·∫øu ch∆∞a t·∫£i, b·ªè comment d√≤ng n√†y ƒë·ªÉ t·∫£i d·ªØ li·ªáu c·∫ßn cho lemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # lemmatizer: T·∫°o m·ªôt ƒë·ªëi t∆∞·ª£ng Lemmatization ƒë·ªÉ t√¨m t·ª´ g·ªëc.\n",
    "\n",
    "# L·ªçc b·ªè c√°c t·ª´ kh√¥ng ph·∫£i ch·ªØ c√°i ho·∫∑c s·ªë (lo·∫°i d·∫•u c√¢u, k√Ω hi·ªáu)\n",
    "filtered_words = [word for word in stopwords_text if word.isalnum()]\n",
    "\n",
    "# Lemmatization\n",
    "print(\"\\nLemmatization:\")\n",
    "for word in filtered_words:\n",
    "    print(f\"{word} -> {lemmatizer.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa71d86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization:\n",
      "machine -> machine\n",
      "learning -> learning\n",
      "method -> method\n",
      "data -> data\n",
      "analysis -> analysis\n",
      "automates -> automates\n",
      "analytical -> analytical\n",
      "model -> model\n",
      "building -> building\n",
      "using -> using\n",
      "algorithms -> algorithm\n",
      "iteratively -> iteratively\n",
      "learn -> learn\n",
      "data -> data\n",
      "machine -> machine\n",
      "learning -> learning\n",
      "allows -> allows\n",
      "computers -> computer\n",
      "find -> find\n",
      "hidden -> hidden\n",
      "insights -> insight\n",
      "without -> without\n",
      "explicitly -> explicitly\n",
      "programmed -> programmed\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "stopwords_text = ['machine', 'learning', 'method', 'data', 'analysis', 'automates', 'analytical', 'model', 'building', '.', 'using', 'algorithms', 'iteratively', 'learn', 'data', ',', 'machine', 'learning', 'allows', 'computers', 'find', 'hidden', 'insights', 'without', 'explicitly', 'programmed', '.', '^-^']\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# L·ªçc b·ªè d·∫•u c√¢u v√† k√Ω hi·ªáu\n",
    "filtered_words = [word for word in stopwords_text if word.isalnum()]\n",
    "\n",
    "# Lemmatize v√† l∆∞u k·∫øt qu·∫£\n",
    "lemmatized_pairs = [f\"{word} -> {lemmatizer.lemmatize(word)}\" for word in filtered_words]\n",
    "\n",
    "# Gh√©p th√†nh ƒëo·∫°n vƒÉn b·∫£n\n",
    "lemmatization_text = \"Lemmatization:\\n\" + \"\\n\".join(lemmatized_pairs)\n",
    "\n",
    "print(lemmatization_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "705aafe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'method', 'data', 'analysis', 'automates', 'analytical', 'model', 'building', 'using', 'algorithm', 'iteratively', 'learn', 'data', 'machine', 'learning', 'allows', 'computer', 'find', 'hidden', 'insight', 'without', 'explicitly', 'programmed']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "#stopwords_text = ['machine', 'learning', 'method', 'data', 'analysis', 'automates', 'analytical', 'model', 'building', '.', 'using', 'algorithms', 'iteratively', 'learn', 'data', ',', 'machine', 'learning', 'allows', 'computers', 'find', 'hidden', 'insights', 'without', 'explicitly', 'programmed', '.', '^-^']\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# L·ªçc v√† lemmatize\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in stopwords_text if word.isalnum()]\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f9017",
   "metadata": {},
   "source": [
    "## Lo·∫°i b·ªè d·∫•u c√¢u v√† k√Ω t·ª± ƒë·∫∑c bi·ªát.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5ec0fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning method data analysis automates analytical model building using algorithm iteratively learn data machine learning allows computer find hidden insight without explicitly programmed\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# G·ªôp danh s√°ch th√†nh chu·ªói\n",
    "text_string = \" \".join(lemmatized_words)\n",
    "\n",
    "clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text_string)\n",
    "\n",
    "print(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef31836",
   "metadata": {},
   "source": [
    "# B√ÄI T·∫¨P 2: X√ÇY D·ª∞NG M√î H√åNH D·ª∞ ƒêO√ÅN (Ch·ªçn m·ªôt trong c√°c ph∆∞∆°ng ph√°p sau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a59036",
   "metadata": {},
   "source": [
    "## Option 1: S·ª≠ d·ª•ng N-Grams ƒë·ªÉ d·ª± ƒëo√°n t·ª´ ti·∫øp theo\n",
    "X√¢y d·ª±ng m√¥ h√¨nh bigram ho·∫∑c trigram t·ª´ ƒëo·∫°n vƒÉn b·∫£n tr√™n.\n",
    "\n",
    "Vi·∫øt code ƒë·ªÉ d·ª± ƒëo√°n t·ª´ ti·∫øp theo sau m·ªôt t·ª´ ho·∫∑c m·ªôt c·ª•m t·ª´ cho tr∆∞·ªõc.\n",
    "\n",
    "V√≠ d·ª•: N·∫øu nh·∫≠p v√†o \"machine learning\", ch∆∞∆°ng tr√¨nh c√≥ th·ªÉ d·ª± ƒëo√°n t·ª´ ti·∫øp theo l√† \"is\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "164e9560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C√°c t·ª´ g·ª£i √Ω ti·∫øp theo k√®m x√°c su·∫•t:\n",
      "learning: 0.0469\n",
      "without: 0.0156\n",
      "automates: 0.0156\n",
      "computers: 0.0156\n",
      "analytical: 0.0156\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ['<s>'] + tokens + ['</s>']\n",
    "\n",
    "def build_ngram_model(tokens, n):\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return Counter(n_grams)\n",
    "\n",
    "def calculate_ngram_probability(model, n_minus_1_model, vocab_size, ngram, k=1):\n",
    "    count_ngram = model.get(ngram, 0) + k\n",
    "    count_prefix = sum(n_minus_1_model.values()) + (vocab_size * k)\n",
    "    return count_ngram / count_prefix\n",
    "\n",
    "def predict_next_word(model, n_minus_1_model, vocab, prefix, k=1):\n",
    "    candidates = {}\n",
    "    for word in vocab:\n",
    "        prob = calculate_ngram_probability(model, n_minus_1_model, len(vocab), prefix + (word,), k)\n",
    "        candidates[word] = prob\n",
    "    return sorted(candidates.items(), key=lambda x: x[1], reverse=True)  # S·∫Øp x·∫øp theo x√°c su·∫•t gi·∫£m d·∫ßn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"Machine learning is a method of data analysis that automates analytical model building. Using algorithms that iteratively learn from data, machine learning allows computers to find hidden insights without being explicitly programmed. ^-^\"\n",
    "    tokens = preprocess_text(text)\n",
    "\n",
    "    bigram_model = build_ngram_model(tokens, 2)\n",
    "    unigram_model = build_ngram_model(tokens, 1)\n",
    "    vocab = set(tokens)\n",
    "\n",
    "    user_input = input(\"Nh·∫≠p m·ªôt t·ª´: \").lower()\n",
    "    suggestions = predict_next_word(bigram_model, unigram_model, vocab, (user_input,))\n",
    "    \n",
    "    print(\"C√°c t·ª´ g·ª£i √Ω ti·∫øp theo k√®m x√°c su·∫•t:\")\n",
    "    for word, prob in suggestions[:5]:  # Hi·ªÉn th·ªã top 5 t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t\n",
    "        print(f\"{word}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f48695",
   "metadata": {},
   "source": [
    "## Option 2: Ph√¢n lo·∫°i vƒÉn b·∫£n b·∫±ng Naive Bayes\n",
    "\n",
    "Gi·∫£ s·ª≠ ƒëo·∫°n vƒÉn b·∫£n tr√™n thu·ªôc m·ªôt t·∫≠p d·ªØ li·ªáu l·ªõn v·ªõi hai nh√£n: \"Tech\" (c√¥ng ngh·ªá) v√† \"Non-Tech\" (kh√¥ng thu·ªôc c√¥ng ngh·ªá)\n",
    "\n",
    "Hu·∫•n luy·ªán m√¥ h√¨nh Naive Bayes v·ªõi m·ªôt t·∫≠p d·ªØ li·ªáu nh·ªè ƒë·ªÉ ph√¢n lo·∫°i vƒÉn b·∫£n.\n",
    "\n",
    "D·ª± ƒëo√°n xem ƒëo·∫°n vƒÉn b·∫£n tr√™n c√≥ thu·ªôc nh√≥m \"Tech\" hay kh√¥ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a95a5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D·ªØ li·ªáu hu·∫•n luy·ªán nh·ªè (gi·∫£ l·∫≠p)\n",
    "\n",
    "train_data = [\n",
    "    (\"Machine learning and artificial intelligence are transforming technology.\", \"Tech\"),\n",
    "    (\"Python and Java are popular programming languages.\", \"Tech\"),\n",
    "    (\"The football match last night was intense and exciting.\", \"Non-Tech\"),\n",
    "    (\"My favorite book is a historical novel about ancient Rome.\", \"Non-Tech\"),\n",
    "    (\"Data science includes machine learning, data mining, and big data.\", \"Tech\"),\n",
    "    (\"I went hiking in the mountains last weekend.\", \"Non-Tech\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f8e8ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ti·ªÅn x·ª≠ l√Ω + tr√≠ch ƒë·∫∑c tr∆∞ng ƒë∆°n gi·∫£n\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# H√†m tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ vƒÉn b·∫£n\n",
    "def extract_features(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    return {word: True for word in words if word.isalpha()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a345c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "\n",
    "# Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu hu·∫•n luy·ªán\n",
    "train_set = [(extract_features(text), label) for text, label in train_data]\n",
    "\n",
    "# Hu·∫•n luy·ªán m√¥ h√¨nh Naive Bayes\n",
    "model = NaiveBayesClassifier.train(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbe6685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå K·∫øt qu·∫£ ph√¢n lo·∫°i: Tech\n"
     ]
    }
   ],
   "source": [
    "# D·ª± ƒëo√°n ƒëo·∫°n vƒÉn b·∫£n cho s·∫µn\n",
    "\n",
    "text = \"Machine learning is a method of data analysis that automates analytical model building. Using algorithms that iteratively learn from data, machine learning allows computers to find hidden insights without being explicitly programmed.\"\n",
    "\n",
    "# Tr√≠ch ƒë·∫∑c tr∆∞ng v√† ph√¢n lo·∫°i\n",
    "features = extract_features(text)\n",
    "predicted = model.classify(features)\n",
    "\n",
    "print(\"üìå K·∫øt qu·∫£ ph√¢n lo·∫°i:\", predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a1787",
   "metadata": {},
   "source": [
    "## Option 3: Ph√¢n lo·∫°i vƒÉn b·∫£n b·∫±ng K-Nearest Neighbors (KNN)\n",
    "\n",
    "Chuy·ªÉn ƒë·ªïi ƒëo·∫°n vƒÉn b·∫£n tr√™n th√†nh m·ªôt vector ƒë·∫∑c tr∆∞ng b·∫±ng ph∆∞∆°ng ph√°p TF-IDF ho·∫∑c Bag-of-Words.\n",
    "\n",
    "S·ª≠ d·ª•ng m√¥ h√¨nh KNN ƒë·ªÉ ph√¢n lo·∫°i vƒÉn b·∫£n v√†o m·ªôt trong hai nh√≥m: \"Tech\" ho·∫∑c \"Non-Tech\".\n",
    "\n",
    "Th·ª≠ nghi·ªám v·ªõi c√°c gi√° tr·ªã k kh√°c nhau v√† ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8dcf9318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Accuracy v·ªõi k=1: 0.67\n",
      "‚úÖ Accuracy v·ªõi k=3: 0.67\n",
      "‚úÖ Accuracy v·ªõi k=5: 0.33\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === 1. D·ªØ li·ªáu m·∫´u ===\n",
    "texts = [\n",
    "    \"Machine learning and artificial intelligence are transforming technology.\",\n",
    "    \"Python and Java are popular programming languages.\",\n",
    "    \"The football match last night was intense and exciting.\",\n",
    "    \"My favorite book is a historical novel about ancient Rome.\",\n",
    "    \"Data science includes machine learning, data mining, and big data.\",\n",
    "    \"I went hiking in the mountains last weekend.\",\n",
    "    \"We used deep learning models to classify images.\",        # Tech\n",
    "    \"I baked a chocolate cake using a new recipe yesterday.\"   # Non-Tech\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"Tech\", \"Tech\",\n",
    "    \"Non-Tech\", \"Non-Tech\",\n",
    "    \"Tech\", \"Non-Tech\",\n",
    "    \"Tech\", \"Non-Tech\"\n",
    "]\n",
    "\n",
    "# === 2. Chuy·ªÉn vƒÉn b·∫£n th√†nh vector TF-IDF ===\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# === 3. Chia train/test ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# === 4. Th·ª≠ nghi·ªám KNN v·ªõi c√°c gi√° tr·ªã k ===\n",
    "for k in [1, 3, 5]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"‚úÖ Accuracy v·ªõi k={k}: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffe8e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå D·ª± ƒëo√°n vƒÉn b·∫£n m·ªõi: Non-Tech\n"
     ]
    }
   ],
   "source": [
    "new_text = [\n",
    "    \"I baked a chocolate cake using a new recipe yesterday. ahihi\"\n",
    "]\n",
    "new_vec = vectorizer.transform(new_text)\n",
    "prediction = knn.predict(new_vec)\n",
    "print(\"üìå D·ª± ƒëo√°n vƒÉn b·∫£n m·ªõi:\", prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033a9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
