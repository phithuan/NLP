{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a84784",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Machine learning is a method of data analysis that automates analytical model building. Using algorithms that iteratively learn from data, machine learning allows computers to find hidden insights without being explicitly programmed. ^-^\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52bb94",
   "metadata": {},
   "source": [
    "# BÀI TẬP 1: TIỀN XỬ LÝ VĂN BẢN (BẮT BUỘC)\n",
    "\n",
    "Viết code để thực hiện tiền xử lý văn bản trên đoạn văn bản trên, bao gồm các bước:\n",
    "\n",
    "- Chuyển tất cả chữ về lowercase.\n",
    "- Tokenization: Chia văn bản thành các từ riêng lẻ.\n",
    "- Loại bỏ stopwords.\n",
    "- Lemmatization để đưa từ về dạng nguyên thể.\n",
    "- Loại bỏ dấu câu và ký tự đặc biệt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42c89f",
   "metadata": {},
   "source": [
    "## Chuyển tất cả chữ về lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fccab52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning is a method of data analysis that automates analytical model building. using algorithms that iteratively learn from data, machine learning allows computers to find hidden insights without being explicitly programmed. ^-^\n"
     ]
    }
   ],
   "source": [
    "# chuyển tất cả về lowercase\n",
    "Lower_text = text.lower()\n",
    "print(Lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc30a01",
   "metadata": {},
   "source": [
    "## Tokenization: Chia văn bản thành các từ riêng lẻ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5e27373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'is', 'a', 'method', 'of', 'data', 'analysis', 'that', 'automates', 'analytical', 'model', 'building', '.', 'using', 'algorithms', 'that', 'iteratively', 'learn', 'from', 'data', ',', 'machine', 'learning', 'allows', 'computers', 'to', 'find', 'hidden', 'insights', 'without', 'being', 'explicitly', 'programmed', '.', '^-^']\n"
     ]
    }
   ],
   "source": [
    "# chia văn bản thành các từ riêng lẻ\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tokenization\n",
    "tokenization_text = word_tokenize(Lower_text)\n",
    "\n",
    "print(tokenization_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30684f7",
   "metadata": {},
   "source": [
    "## Loại bỏ stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fdbd9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "--------------------\n",
      "['machine', 'learning', 'is', 'a', 'method', 'of', 'data', 'analysis', 'that', 'automates', 'analytical', 'model', 'building', '.', 'using', 'algorithms', 'that', 'iteratively', 'learn', 'from', 'data', ',', 'machine', 'learning', 'allows', 'computers', 'to', 'find', 'hidden', 'insights', 'without', 'being', 'explicitly', 'programmed', '.', '^-^']\n",
      "['machine', 'learning', 'method', 'data', 'analysis', 'automates', 'analytical', 'model', 'building', '.', 'using', 'algorithms', 'iteratively', 'learn', 'data', ',', 'machine', 'learning', 'allows', 'computers', 'find', 'hidden', 'insights', 'without', 'explicitly', 'programmed', '.', '^-^']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport nltk\\n# nltk.download(\\'stopwords\\')  # Bỏ comment dòng này nếu bạn chưa tải stopwords\\n\\n# Lấy danh sách stopwords tiếng Anh\\nenglish_stopwords = nltk.corpus.stopwords.words(\"english\")\\n\\n# Hàm loại bỏ stopwords\\ndef remove_stopwords(tokens):\\n    return [token for token in tokens if token not in english_stopwords]\\n\\n# Ví dụ sử dụng\\ntokens = [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\\nfiltered = remove_stopwords(tokens)\\nprint(\"Filtered tokens:\", filtered)\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE 4.3\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "english_stopwords = nltk.corpus.stopwords.words(\"english\") # nltk.corpus.stopwords: Truy cập tập hợp từ dừng có sẵn trong NLTK.\n",
    "                                                           # .words(\"english\"): Lấy danh sách stopwords của tiếng Anh.\n",
    "print(len(english_stopwords))\n",
    "print(english_stopwords)\n",
    "print(\"-\"*20)\n",
    "\n",
    "def remove_stopwords(tokens, language = \"english\"):\n",
    "    stopword_list = nltk.corpus.stopwords.words(language)\n",
    "    filter_tokens = [token for token in tokens if token not in stopword_list] #Duyệt qua từng token trong tokens\n",
    "                                                                              # Chỉ giữ lại những từ không nằm trong stopword_list\n",
    "    return filter_tokens\n",
    "\n",
    "print(tokenization_text)\n",
    "#nltk.download('stopwords')\n",
    "stopwords_text = remove_stopwords(tokenization_text)\n",
    "print(stopwords_text)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import nltk\n",
    "# nltk.download('stopwords')  # Bỏ comment dòng này nếu bạn chưa tải stopwords\n",
    "\n",
    "# Lấy danh sách stopwords tiếng Anh\n",
    "english_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Hàm loại bỏ stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in english_stopwords]\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "tokens = [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\n",
    "filtered = remove_stopwords(tokens)\n",
    "print(\"Filtered tokens:\", filtered)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda12799",
   "metadata": {},
   "source": [
    "## Lemmatization để đưa từ về dạng nguyên thể.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c0b869a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization:\n",
      "machine -> machine\n",
      "learning -> learning\n",
      "method -> method\n",
      "data -> data\n",
      "analysis -> analysis\n",
      "automates -> automates\n",
      "analytical -> analytical\n",
      "model -> model\n",
      "building -> building\n",
      "using -> using\n",
      "algorithms -> algorithm\n",
      "iteratively -> iteratively\n",
      "learn -> learn\n",
      "data -> data\n",
      "machine -> machine\n",
      "learning -> learning\n",
      "allows -> allows\n",
      "computers -> computer\n",
      "find -> find\n",
      "hidden -> hidden\n",
      "insights -> insight\n",
      "without -> without\n",
      "explicitly -> explicitly\n",
      "programmed -> programmed\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization để đưa từ về dạng nguyên thể.\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "# nltk.download('wordnet')  # Nếu chưa tải, bỏ comment dòng này để tải dữ liệu cần cho lemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # lemmatizer: Tạo một đối tượng Lemmatization để tìm từ gốc.\n",
    "\n",
    "# Lọc bỏ các từ không phải chữ cái hoặc số (loại dấu câu, ký hiệu)\n",
    "filtered_words = [word for word in stopwords_text if word.isalnum()]\n",
    "\n",
    "# Lemmatization\n",
    "print(\"\\nLemmatization:\")\n",
    "for word in filtered_words:\n",
    "    print(f\"{word} -> {lemmatizer.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa71d86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization:\n",
      "machine -> machine\n",
      "learning -> learning\n",
      "method -> method\n",
      "data -> data\n",
      "analysis -> analysis\n",
      "automates -> automates\n",
      "analytical -> analytical\n",
      "model -> model\n",
      "building -> building\n",
      "using -> using\n",
      "algorithms -> algorithm\n",
      "iteratively -> iteratively\n",
      "learn -> learn\n",
      "data -> data\n",
      "machine -> machine\n",
      "learning -> learning\n",
      "allows -> allows\n",
      "computers -> computer\n",
      "find -> find\n",
      "hidden -> hidden\n",
      "insights -> insight\n",
      "without -> without\n",
      "explicitly -> explicitly\n",
      "programmed -> programmed\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "stopwords_text = ['machine', 'learning', 'method', 'data', 'analysis', 'automates', 'analytical', 'model', 'building', '.', 'using', 'algorithms', 'iteratively', 'learn', 'data', ',', 'machine', 'learning', 'allows', 'computers', 'find', 'hidden', 'insights', 'without', 'explicitly', 'programmed', '.', '^-^']\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lọc bỏ dấu câu và ký hiệu\n",
    "filtered_words = [word for word in stopwords_text if word.isalnum()]\n",
    "\n",
    "# Lemmatize và lưu kết quả\n",
    "lemmatized_pairs = [f\"{word} -> {lemmatizer.lemmatize(word)}\" for word in filtered_words]\n",
    "\n",
    "# Ghép thành đoạn văn bản\n",
    "lemmatization_text = \"Lemmatization:\\n\" + \"\\n\".join(lemmatized_pairs)\n",
    "\n",
    "print(lemmatization_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "705aafe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'method', 'data', 'analysis', 'automates', 'analytical', 'model', 'building', 'using', 'algorithm', 'iteratively', 'learn', 'data', 'machine', 'learning', 'allows', 'computer', 'find', 'hidden', 'insight', 'without', 'explicitly', 'programmed']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "#stopwords_text = ['machine', 'learning', 'method', 'data', 'analysis', 'automates', 'analytical', 'model', 'building', '.', 'using', 'algorithms', 'iteratively', 'learn', 'data', ',', 'machine', 'learning', 'allows', 'computers', 'find', 'hidden', 'insights', 'without', 'explicitly', 'programmed', '.', '^-^']\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lọc và lemmatize\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in stopwords_text if word.isalnum()]\n",
    "\n",
    "# In kết quả\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f9017",
   "metadata": {},
   "source": [
    "## Loại bỏ dấu câu và ký tự đặc biệt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5ec0fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning method data analysis automates analytical model building using algorithm iteratively learn data machine learning allows computer find hidden insight without explicitly programmed\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Gộp danh sách thành chuỗi\n",
    "text_string = \" \".join(lemmatized_words)\n",
    "\n",
    "clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text_string)\n",
    "\n",
    "print(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef31836",
   "metadata": {},
   "source": [
    "# BÀI TẬP 2: XÂY DỰNG MÔ HÌNH DỰ ĐOÁN (Chọn một trong các phương pháp sau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a59036",
   "metadata": {},
   "source": [
    "## Option 1: Sử dụng N-Grams để dự đoán từ tiếp theo\n",
    "Xây dựng mô hình bigram hoặc trigram từ đoạn văn bản trên.\n",
    "\n",
    "Viết code để dự đoán từ tiếp theo sau một từ hoặc một cụm từ cho trước.\n",
    "\n",
    "Ví dụ: Nếu nhập vào \"machine learning\", chương trình có thể dự đoán từ tiếp theo là \"is\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "164e9560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các từ gợi ý tiếp theo kèm xác suất:\n",
      "learning: 0.0469\n",
      "without: 0.0156\n",
      "automates: 0.0156\n",
      "computers: 0.0156\n",
      "analytical: 0.0156\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ['<s>'] + tokens + ['</s>']\n",
    "\n",
    "def build_ngram_model(tokens, n):\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return Counter(n_grams)\n",
    "\n",
    "def calculate_ngram_probability(model, n_minus_1_model, vocab_size, ngram, k=1):\n",
    "    count_ngram = model.get(ngram, 0) + k\n",
    "    count_prefix = sum(n_minus_1_model.values()) + (vocab_size * k)\n",
    "    return count_ngram / count_prefix\n",
    "\n",
    "def predict_next_word(model, n_minus_1_model, vocab, prefix, k=1):\n",
    "    candidates = {}\n",
    "    for word in vocab:\n",
    "        prob = calculate_ngram_probability(model, n_minus_1_model, len(vocab), prefix + (word,), k)\n",
    "        candidates[word] = prob\n",
    "    return sorted(candidates.items(), key=lambda x: x[1], reverse=True)  # Sắp xếp theo xác suất giảm dần\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"Machine learning is a method of data analysis that automates analytical model building. Using algorithms that iteratively learn from data, machine learning allows computers to find hidden insights without being explicitly programmed. ^-^\"\n",
    "    tokens = preprocess_text(text)\n",
    "\n",
    "    bigram_model = build_ngram_model(tokens, 2)\n",
    "    unigram_model = build_ngram_model(tokens, 1)\n",
    "    vocab = set(tokens)\n",
    "\n",
    "    user_input = input(\"Nhập một từ: \").lower()\n",
    "    suggestions = predict_next_word(bigram_model, unigram_model, vocab, (user_input,))\n",
    "    \n",
    "    print(\"Các từ gợi ý tiếp theo kèm xác suất:\")\n",
    "    for word, prob in suggestions[:5]:  # Hiển thị top 5 từ có xác suất cao nhất\n",
    "        print(f\"{word}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f48695",
   "metadata": {},
   "source": [
    "## Option 2: Phân loại văn bản bằng Naive Bayes\n",
    "\n",
    "Giả sử đoạn văn bản trên thuộc một tập dữ liệu lớn với hai nhãn: \"Tech\" (công nghệ) và \"Non-Tech\" (không thuộc công nghệ)\n",
    "\n",
    "Huấn luyện mô hình Naive Bayes với một tập dữ liệu nhỏ để phân loại văn bản.\n",
    "\n",
    "Dự đoán xem đoạn văn bản trên có thuộc nhóm \"Tech\" hay không."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a95a5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dữ liệu huấn luyện nhỏ (giả lập)\n",
    "\n",
    "train_data = [\n",
    "    (\"Machine learning and artificial intelligence are transforming technology.\", \"Tech\"),\n",
    "    (\"Python and Java are popular programming languages.\", \"Tech\"),\n",
    "    (\"The football match last night was intense and exciting.\", \"Non-Tech\"),\n",
    "    (\"My favorite book is a historical novel about ancient Rome.\", \"Non-Tech\"),\n",
    "    (\"Data science includes machine learning, data mining, and big data.\", \"Tech\"),\n",
    "    (\"I went hiking in the mountains last weekend.\", \"Non-Tech\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f8e8ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tiền xử lý + trích đặc trưng đơn giản\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Hàm trích xuất đặc trưng từ văn bản\n",
    "def extract_features(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    return {word: True for word in words if word.isalpha()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a345c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Huấn luyện mô hình\n",
    "\n",
    "# Tiền xử lý dữ liệu huấn luyện\n",
    "train_set = [(extract_features(text), label) for text, label in train_data]\n",
    "\n",
    "# Huấn luyện mô hình Naive Bayes\n",
    "model = NaiveBayesClassifier.train(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbe6685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Kết quả phân loại: Tech\n"
     ]
    }
   ],
   "source": [
    "# Dự đoán đoạn văn bản cho sẵn\n",
    "\n",
    "text = \"Machine learning is a method of data analysis that automates analytical model building. Using algorithms that iteratively learn from data, machine learning allows computers to find hidden insights without being explicitly programmed.\"\n",
    "\n",
    "# Trích đặc trưng và phân loại\n",
    "features = extract_features(text)\n",
    "predicted = model.classify(features)\n",
    "\n",
    "print(\"📌 Kết quả phân loại:\", predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a1787",
   "metadata": {},
   "source": [
    "## Option 3: Phân loại văn bản bằng K-Nearest Neighbors (KNN)\n",
    "\n",
    "Chuyển đổi đoạn văn bản trên thành một vector đặc trưng bằng phương pháp TF-IDF hoặc Bag-of-Words.\n",
    "\n",
    "Sử dụng mô hình KNN để phân loại văn bản vào một trong hai nhóm: \"Tech\" hoặc \"Non-Tech\".\n",
    "\n",
    "Thử nghiệm với các giá trị k khác nhau và đánh giá độ chính xác."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8dcf9318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy với k=1: 0.67\n",
      "✅ Accuracy với k=3: 0.67\n",
      "✅ Accuracy với k=5: 0.33\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === 1. Dữ liệu mẫu ===\n",
    "texts = [\n",
    "    \"Machine learning and artificial intelligence are transforming technology.\",\n",
    "    \"Python and Java are popular programming languages.\",\n",
    "    \"The football match last night was intense and exciting.\",\n",
    "    \"My favorite book is a historical novel about ancient Rome.\",\n",
    "    \"Data science includes machine learning, data mining, and big data.\",\n",
    "    \"I went hiking in the mountains last weekend.\",\n",
    "    \"We used deep learning models to classify images.\",        # Tech\n",
    "    \"I baked a chocolate cake using a new recipe yesterday.\"   # Non-Tech\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"Tech\", \"Tech\",\n",
    "    \"Non-Tech\", \"Non-Tech\",\n",
    "    \"Tech\", \"Non-Tech\",\n",
    "    \"Tech\", \"Non-Tech\"\n",
    "]\n",
    "\n",
    "# === 2. Chuyển văn bản thành vector TF-IDF ===\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# === 3. Chia train/test ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# === 4. Thử nghiệm KNN với các giá trị k ===\n",
    "for k in [1, 3, 5]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"✅ Accuracy với k={k}: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffe8e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Dự đoán văn bản mới: Non-Tech\n"
     ]
    }
   ],
   "source": [
    "new_text = [\n",
    "    \"I baked a chocolate cake using a new recipe yesterday. ahihi\"\n",
    "]\n",
    "new_vec = vectorizer.transform(new_text)\n",
    "prediction = knn.predict(new_vec)\n",
    "print(\"📌 Dự đoán văn bản mới:\", prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033a9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
