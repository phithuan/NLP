{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TH·ª∞C H√ÄNH BU·ªîI 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Th·ª±c h√†nh code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading punk_tab: Package 'punk_tab' not found in\n",
      "[nltk_data]     index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c√°c th∆∞ vi·ªán c·∫ßn\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punk_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Chuy·ªÉn ƒë·ªïi t·∫•t c·∫£ c√°c k√Ω t·ª± trong chu·ªói th√†nh ch·ªØ th∆∞·ªùng\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Lo·∫°i b·ªè t·∫•t c·∫£ d·∫•u c√¢u, ch·ªâ gi·ªØ l·∫°i ch·ªØ c√°i, s·ªë v√† kho·∫£ng tr·∫Øng\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \"\"\"\\w: C√°c k√Ω t·ª± ch·ªØ c√°i v√† s·ªë.s\n",
    "    \t\\s: Kho·∫£ng tr·∫Øng.\n",
    "        [^\\w\\s]: B·∫•t k·ª≥ k√Ω t·ª± n√†o kh√¥ng ph·∫£i ch·ªØ, s·ªë ho·∫∑c kho·∫£ng tr·∫Øng s·∫Ω b·ªã lo·∫°i b·ªè (v√≠ d·ª•: d·∫•u ch·∫•m c√¢u).\"\"\"\n",
    "    # Tokenize (t√°ch vƒÉn b·∫£n th√†nh c√°c t·ª´ ri√™ng l·∫ª)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Th√™m token ƒë·∫∑c bi·ªát v√†o ƒë·∫ßu v√† cu·ªëi c√¢u\n",
    "    tokens = ['<s>'] + tokens + ['</s>']\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def build_ngram_model(tokens, n):\n",
    "    \"\"\"\n",
    "    X√¢y d·ª±ng m√¥ h√¨nh n-gram t·ª´ danh s√°ch tokens.\n",
    "    \n",
    "    Parameters:\n",
    "        tokens (list): Danh s√°ch c√°c t·ª´ ƒë√£ ti·ªÅn x·ª≠ l√Ω.\n",
    "        n (int): B·∫≠c c·ªßa n-gram (unigram, bigram, trigram,...).\n",
    "    \n",
    "    Returns:\n",
    "        Counter: M·ªôt dictionary ch·ª©a n-grams v√† s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa ch√∫ng.\n",
    "    \"\"\"\n",
    "    n_grams = list(ngrams(tokens, n))  # T·∫°o danh s√°ch c√°c n-gram t·ª´ danh s√°ch tokens\n",
    "    model = Counter(n_grams)  # ƒê·∫øm t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa t·ª´ng n-gram\n",
    "    return model  # Tr·∫£ v·ªÅ m√¥ h√¨nh n-gram\n",
    "\n",
    "\n",
    "def calculate_ngram_probability(model, n_minus_1_model, vocab_size, ngram, k=1):\n",
    "    \"\"\"\n",
    "    T√≠nh x√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán c·ªßa m·ªôt n-gram d·ª±a tr√™n m√¥ h√¨nh n-gram v√† Laplace smoothing.\n",
    "    \n",
    "    Parameters:\n",
    "        model (Counter): M√¥ h√¨nh n-gram ·ªü b·∫≠c n (v√≠ d·ª•: bigram, trigram).\n",
    "        n_minus_1_model (Counter): M√¥ h√¨nh n-gram ·ªü b·∫≠c (n-1) (v√≠ d·ª•: unigram n·∫øu ƒëang x√©t bigram).\n",
    "        vocab_size (int): K√≠ch th∆∞·ªõc t·ª´ v·ª±ng c·ªßa t·∫≠p d·ªØ li·ªáu.\n",
    "        ngram (tuple): M·ªôt n-gram c·∫ßn t√≠nh x√°c su·∫•t.\n",
    "        k (int, optional): H·ªá s·ªë ƒëi·ªÅu ch·ªânh Laplace smoothing (default = 1).\n",
    "    \n",
    "    Returns:\n",
    "        float: X√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán c·ªßa n-gram.\n",
    "    \"\"\"\n",
    "    count_ngram = model.get(ngram, 0) + k  # S·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa n-gram (th√™m k ƒë·ªÉ tr√°nh x√°c su·∫•t b·∫±ng 0)\n",
    "    \n",
    "    prefix = ngram[:-1]  # Ph·∫ßn ti·ªÅn t·ªë c·ªßa n-gram (v√≠ d·ª•: v·ªõi trigram ('a', 'b', 'c') -> prefix ('a', 'b'))\n",
    "    count_prefix = n_minus_1_model.get(prefix, 0) + (vocab_size * k)  # S·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa ti·ªÅn t·ªë (√°p d·ª•ng Laplace)\n",
    "\n",
    "    return count_ngram / count_prefix  # X√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán c·ªßa n-gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, n_minus_1_model, vocab, prefix, n, k=1):\n",
    "    # Ki·ªÉm tra xem ti·ªÅn t·ªë (prefix) c√≥ t·ªìn t·∫°i trong m√¥ h√¨nh (n-1)-gram kh√¥ng\n",
    "    if prefix not in n_minus_1_model:\n",
    "        return None  # N·∫øu kh√¥ng c√≥, kh√¥ng th·ªÉ d·ª± ƒëo√°n -> tr·∫£ v·ªÅ None\n",
    "    \n",
    "    candidates = {}  # T·∫°o dictionary ƒë·ªÉ l∆∞u x√°c su·∫•t c·ªßa c√°c t·ª´ ti·∫øp theo\n",
    "    \n",
    "    # Duy·ªát qua t·ª´ng t·ª´ trong t·ª´ v·ª±ng\n",
    "    for word in vocab:\n",
    "        # T√≠nh x√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán c·ªßa n-gram d·ª±a tr√™n ti·ªÅn t·ªë v√† t·ª´ hi·ªán t·∫°i\n",
    "        prob = calculate_ngram_probability(model, n_minus_1_model, len(vocab), prefix + (word,), k)\n",
    "        candidates[word] = prob  # L∆∞u x√°c su·∫•t c·ªßa t·ª´ v√†o dictionary\n",
    "    \n",
    "    # Tr·∫£ v·ªÅ t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t (n·∫øu c√≥ t·ª´ h·ª£p l·ªá)\n",
    "    return max(candidates, key=candidates.get) if candidates else None\n",
    "\n",
    "\"\"\"üîπ C√°ch ho·∫°t ƒë·ªông\n",
    "Ki·ªÉm tra xem ti·ªÅn t·ªë (prefix) c√≥ trong m√¥ h√¨nh (n-1)-gram kh√¥ng\n",
    "\n",
    "N·∫øu kh√¥ng c√≥, tr·∫£ v·ªÅ None (v√¨ kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ d·ª± ƒëo√°n).\n",
    "\n",
    "Duy·ªát qua t·∫•t c·∫£ c√°c t·ª´ trong vocab\n",
    "\n",
    "T√≠nh x√°c su·∫•t c·ªßa t·ª´ng t·ª´ d·ª±a tr√™n calculate_gram_probability.\n",
    "\n",
    "L∆∞u x√°c su·∫•t v√†o candidates.\n",
    "\n",
    "Ch·ªçn t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t\n",
    "\n",
    "Tr·∫£ v·ªÅ t·ª´ c√≥ gi√° tr·ªã l·ªõn nh·∫•t trong candidates (n·∫øu danh s√°ch kh√¥ng r·ªóng).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, n_minus_1_model, test_tokens, n, vocab_size, k=1):\n",
    "    # Ki·ªÉm tra n·∫øu danh s√°ch test_tokens r·ªóng, tr·∫£ v·ªÅ v√¥ c·ª±c (inf) ƒë·ªÉ tr√°nh l·ªói t√≠nh to√°n\n",
    "    if not test_tokens:\n",
    "        return float('inf')  \n",
    "    \n",
    "    # T·∫°o danh s√°ch c√°c n-grams t·ª´ t·∫≠p d·ªØ li·ªáu ki·ªÉm tra\n",
    "    test_ngrams = list(ngrams(test_tokens, n))\n",
    "    \n",
    "    # N·∫øu kh√¥ng c√≥ n-gram n√†o ƒë∆∞·ª£c t·∫°o ra, tr·∫£ v·ªÅ v√¥ c·ª±c\n",
    "    if not test_ngrams:\n",
    "        return float('inf') \n",
    "    \n",
    "    log_prob = 0  # Kh·ªüi t·∫°o t·ªïng log x√°c su·∫•t\n",
    "    \n",
    "    # Duy·ªát qua t·ª´ng n-gram trong t·∫≠p ki·ªÉm tra\n",
    "    for ngram in test_ngrams:\n",
    "        # T√≠nh x√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán c·ªßa n-gram\n",
    "        prob = calculate_ngram_probability(model, n_minus_1_model, vocab_size, ngram, k)\n",
    "        \n",
    "        # N·∫øu x√°c su·∫•t > 0, l·∫•y log b√¨nh th∆∞·ªùng\n",
    "        if prob > 0:\n",
    "            log_prob += math.log(prob)\n",
    "        else:\n",
    "            # Tr√°nh log(0) b·∫±ng c√°ch thay th·∫ø b·∫±ng gi√° tr·ªã r·∫•t nh·ªè (1e-10)\n",
    "            log_prob += math.log(1e-10)  \n",
    "\n",
    "    # T√≠nh perplexity\n",
    "    return math.exp(-log_prob / len(test_ngrams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram model: Counter({('h√¥m', 'nay'): 2, ('th·ª±c', 'h√†nh'): 2, ('h√†nh', 'nlp'): 2, ('<s>', 'h√¥m'): 1, ('nay', 'tr·ªùi'): 1, ('tr·ªùi', 'ƒë·∫πp'): 1, ('ƒë·∫πp', 'h√¥m'): 1, ('nay', 't√¥i'): 1, ('t√¥i', 'ƒëi'): 1, ('ƒëi', 'th·ª±c'): 1, ('nlp', '·ªü'): 1, ('·ªü', 'ph√≤ng'): 1, ('ph√≤ng', 'm√°y'): 1, ('m√°y', '10'): 1, ('10', 'th·ª±c'): 1, ('nlp', 'r·∫•t'): 1, ('r·∫•t', 'th√∫'): 1, ('th√∫', 'v·ªã'): 1, ('v·ªã', '</s>'): 1})\n",
      "T·ª´ d·ª± ƒëo√°n ti·∫øp theo sau 't√¥i': ƒëi\n",
      "Perplexity c·ªßa m√¥ h√¨nh: 18.493242008906932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # T·∫£i xu·ªëng b·ªô tokenizer 'punkt' t·ª´ NLTK n·∫øu ch∆∞a c√≥\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "    # Kh·ªüi t·∫°o vƒÉn b·∫£n ƒë·∫ßu v√†o\n",
    "    text = \"h√¥m nay tr·ªùi ƒë·∫πp. h√¥m nay t√¥i ƒëi th·ª±c h√†nh NLP ·ªü ph√≤ng m√°y 10. Th·ª±c h√†nh NLP r·∫•t th√∫ v·ªã\"\n",
    "    \n",
    "    # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng, lo·∫°i b·ªè d·∫•u c√¢u, tokenization\n",
    "    tokens = preprocess_text(text)\n",
    "\n",
    "    # X√¢y d·ª±ng m√¥ h√¨nh bigram (n-gram v·ªõi n=2)\n",
    "    bigram_model = build_ngram_model(tokens, 2)\n",
    "\n",
    "    # X√¢y d·ª±ng m√¥ h√¨nh unigram (n-gram v·ªõi n=1)\n",
    "    unigram_model = build_ngram_model(tokens, 1)\n",
    "\n",
    "    # T·∫°o t·∫≠p t·ª´ v·ª±ng t·ª´ danh s√°ch token\n",
    "    vocab = set(tokens)\n",
    "\n",
    "    # In m√¥ h√¨nh bigram ƒë√£ t·∫°o\n",
    "    print(\"Bigram model:\", bigram_model)\n",
    "\n",
    "    # D·ª± ƒëo√°n t·ª´ ti·∫øp theo sau t·ª´ 't√¥i' b·∫±ng m√¥ h√¨nh bigram\n",
    "    next_word = predict_next_word(bigram_model, unigram_model, vocab, ('t√¥i',), 2)\n",
    "    print(\"T·ª´ d·ª± ƒëo√°n ti·∫øp theo sau 't√¥i':\", next_word)\n",
    "\n",
    "    # VƒÉn b·∫£n th·ª≠ nghi·ªám ƒë·ªÉ t√≠nh perplexity\n",
    "    test_text = \"t√¥i th√≠ch AI\"\n",
    "\n",
    "    # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n th·ª≠ nghi·ªám\n",
    "    test_tokens = preprocess_text(test_text)\n",
    "\n",
    "    # T√≠nh perplexity c·ªßa m√¥ h√¨nh tr√™n vƒÉn b·∫£n th·ª≠ nghi·ªám\n",
    "    perplexity = calculate_perplexity(bigram_model, unigram_model, test_tokens, 2, len(vocab))\n",
    "    print(\"Perplexity c·ªßa m√¥ h√¨nh:\", perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ƒê·ªàNH CAO C√îNG NGH·ªÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C√°c t·ª´ g·ª£i √Ω ti·∫øp theo k√®m x√°c su·∫•t:\n",
      "nay: 0.0244\n",
      "ƒëi: 0.0244\n",
      "<s>: 0.0244\n",
      "h√†nh: 0.0244\n",
      "r·∫•t: 0.0244\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ['<s>'] + tokens + ['</s>']\n",
    "\n",
    "def build_ngram_model(tokens, n):\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return Counter(n_grams)\n",
    "\n",
    "def calculate_ngram_probability(model, n_minus_1_model, vocab_size, ngram, k=1):\n",
    "    count_ngram = model.get(ngram, 0) + k\n",
    "    count_prefix = sum(n_minus_1_model.values()) + (vocab_size * k)\n",
    "    return count_ngram / count_prefix\n",
    "\n",
    "def predict_next_word(model, n_minus_1_model, vocab, prefix, k=1):\n",
    "    candidates = {}\n",
    "    for word in vocab:\n",
    "        prob = calculate_ngram_probability(model, n_minus_1_model, len(vocab), prefix + (word,), k)\n",
    "        candidates[word] = prob\n",
    "    return sorted(candidates.items(), key=lambda x: x[1], reverse=True)  # S·∫Øp x·∫øp theo x√°c su·∫•t gi·∫£m d·∫ßn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"h√¥m nay tr·ªùi ƒë·∫πp. h√¥m nay t√¥i ƒëi th·ª±c h√†nh NLP ·ªü ph√≤ng m√°y 10. Th·ª±c h√†nh NLP r·∫•t th√∫ v·ªã\"\n",
    "    tokens = preprocess_text(text)\n",
    "\n",
    "    bigram_model = build_ngram_model(tokens, 2)\n",
    "    unigram_model = build_ngram_model(tokens, 1)\n",
    "    vocab = set(tokens)\n",
    "\n",
    "    user_input = input(\"Nh·∫≠p m·ªôt t·ª´: \").lower()\n",
    "    suggestions = predict_next_word(bigram_model, unigram_model, vocab, (user_input,))\n",
    "    \n",
    "    print(\"C√°c t·ª´ g·ª£i √Ω ti·∫øp theo k√®m x√°c su·∫•t:\")\n",
    "    for word, prob in suggestions[:5]:  # Hi·ªÉn th·ªã top 5 t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t\n",
    "        print(f\"{word}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def build_ngram_model(tokens, n):\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    model = Counter(n_grams)\n",
    "    return model\n",
    "\n",
    "def predict_next_word(model, prefix):\n",
    "    candidates = {k[-1]: v for k, v in model.items() if k[:-1] == prefix}\n",
    "    return sorted(candidates.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "def update_suggestions(event=None):\n",
    "    prefix = entry_var.get().strip().split()\n",
    "    if not prefix:\n",
    "        suggestion_list.delete(0, tk.END)\n",
    "        return\n",
    "\n",
    "    prefix = tuple(prefix[-1:])\n",
    "    suggestions = predict_next_word(bigram_model, prefix)\n",
    "\n",
    "    suggestion_list.delete(0, tk.END)\n",
    "    for word, _ in suggestions:\n",
    "        suggestion_list.insert(tk.END, word)\n",
    "\n",
    "    if suggestion_list.size() > 0:\n",
    "        suggestion_list.selection_set(0)\n",
    "        suggestion_list.activate(0)\n",
    "\n",
    "def on_tab(event):\n",
    "    if suggestion_list.size() > 0:\n",
    "        try:\n",
    "            index = suggestion_list.curselection()[0]\n",
    "            selected_word = suggestion_list.get(index)\n",
    "        except IndexError:\n",
    "            selected_word = suggestion_list.get(0)\n",
    "\n",
    "        current_text = entry_var.get().strip()\n",
    "        new_text = f\"{current_text} {selected_word}\"\n",
    "        entry_var.set(new_text)\n",
    "        entry.icursor(tk.END)\n",
    "        suggestion_list.delete(0, tk.END)\n",
    "        return \"break\"\n",
    "\n",
    "def on_arrow_key(event):\n",
    "    if suggestion_list.size() == 0:\n",
    "        return\n",
    "\n",
    "    index = suggestion_list.curselection()\n",
    "    if not index:\n",
    "        current_index = 0\n",
    "    else:\n",
    "        current_index = index[0]\n",
    "\n",
    "    if event.keysym == \"Down\":\n",
    "        new_index = min(current_index + 1, suggestion_list.size() - 1)\n",
    "    elif event.keysym == \"Up\":\n",
    "        new_index = max(current_index - 1, 0)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    suggestion_list.selection_clear(0, tk.END)\n",
    "    suggestion_list.selection_set(new_index)\n",
    "    suggestion_list.activate(new_index)\n",
    "    return \"break\"  # NgƒÉn m·∫∑c ƒë·ªãnh di chuy·ªÉn con tr·ªè\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ file\n",
    "with open(\"data_ctut.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "tokens = preprocess_text(text)\n",
    "bigram_model = build_ngram_model(tokens, 2)\n",
    "\n",
    "# Giao di·ªán Tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"G·ª£i √Ω t·ª´\")\n",
    "\n",
    "entry_var = tk.StringVar()\n",
    "entry = tk.Entry(root, textvariable=entry_var, font=(\"Arial\", 14), width=50)\n",
    "entry.pack(padx=10, pady=5)\n",
    "\n",
    "suggestion_list = tk.Listbox(root, font=(\"Arial\", 14), width=50)\n",
    "suggestion_list.pack(padx=10, pady=5)\n",
    "\n",
    "# G·∫Øn s·ª± ki·ªán\n",
    "entry.bind(\"<KeyRelease>\", update_suggestions)\n",
    "entry.bind(\"<Tab>\", on_tab)\n",
    "entry.bind(\"<Up>\", on_arrow_key)\n",
    "entry.bind(\"<Down>\", on_arrow_key)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ========== X·ª≠ l√Ω ng√¥n ng·ªØ ==========\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def build_ngram_model(tokens, n):\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return Counter(n_grams)\n",
    "\n",
    "def predict_next_word(model, prefix):\n",
    "    prefix = tuple(prefix)\n",
    "    candidates = {k[-1]: v for k, v in model.items() if k[:-1] == prefix}\n",
    "    return sorted(candidates.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# ========== H√†m s·ª± ki·ªán ==========\n",
    "def update_suggestions(event=None):\n",
    "    words = entry_var.get().strip().split()\n",
    "    if not words or len(words) < n - 1:\n",
    "        suggestion_list.delete(0, tk.END)\n",
    "        return\n",
    "\n",
    "    prefix = words[-(n - 1):]\n",
    "    suggestions = predict_next_word(ngram_model, prefix)\n",
    "\n",
    "    suggestion_list.delete(0, tk.END)\n",
    "    for word, _ in suggestions:\n",
    "        suggestion_list.insert(tk.END, word)\n",
    "\n",
    "    if suggestion_list.size() > 0:\n",
    "        suggestion_list.selection_set(0)\n",
    "        suggestion_list.activate(0)\n",
    "\n",
    "def on_tab(event):\n",
    "    if suggestion_list.size() == 0:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        index = suggestion_list.curselection()[0]\n",
    "    except IndexError:\n",
    "        index = 0\n",
    "\n",
    "    selected_word = suggestion_list.get(index)\n",
    "    current_text = entry_var.get().strip()\n",
    "    new_text = f\"{current_text} {selected_word}\"\n",
    "    entry_var.set(new_text)\n",
    "    entry.icursor(tk.END)\n",
    "    suggestion_list.delete(0, tk.END)\n",
    "    return \"break\"\n",
    "\n",
    "def on_arrow_key(event):\n",
    "    if suggestion_list.size() == 0:\n",
    "        return\n",
    "\n",
    "    index = suggestion_list.curselection()\n",
    "    current_index = index[0] if index else 0\n",
    "\n",
    "    if event.keysym == \"Down\":\n",
    "        new_index = min(current_index + 1, suggestion_list.size() - 1)\n",
    "    elif event.keysym == \"Up\":\n",
    "        new_index = max(current_index - 1, 0)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    suggestion_list.selection_clear(0, tk.END)\n",
    "    suggestion_list.selection_set(new_index)\n",
    "    suggestion_list.activate(new_index)\n",
    "    return \"break\"\n",
    "\n",
    "def on_click_suggestion(event):\n",
    "    try:\n",
    "        index = suggestion_list.curselection()[0]\n",
    "        selected_word = suggestion_list.get(index)\n",
    "        current_text = entry_var.get().strip()\n",
    "        new_text = f\"{current_text} {selected_word}\"\n",
    "        entry_var.set(new_text)\n",
    "        entry.icursor(tk.END)\n",
    "        suggestion_list.delete(0, tk.END)\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "# ========== ƒê·ªçc d·ªØ li·ªáu v√† t·∫°o m√¥ h√¨nh ==========\n",
    "with open(\"data_ctut.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "tokens = preprocess_text(text)\n",
    "n = 2  # ƒê·ªïi th√†nh 3 n·∫øu mu·ªën d√πng trigram\n",
    "ngram_model = build_ngram_model(tokens, n)\n",
    "\n",
    "# ========== Giao di·ªán Tkinter ==========\n",
    "root = tk.Tk()\n",
    "root.title(\"G·ª£i √Ω t·ª´ th√¥ng minh\")\n",
    "root.geometry(\"600x300\")\n",
    "root.resizable(False, False)\n",
    "\n",
    "# Entry\n",
    "entry_var = tk.StringVar()\n",
    "entry = tk.Entry(root, textvariable=entry_var, font=(\"Arial\", 14), width=50)\n",
    "entry.pack(pady=10)\n",
    "\n",
    "# Scrollbar + Listbox\n",
    "frame = tk.Frame(root)\n",
    "frame.pack()\n",
    "\n",
    "scrollbar = tk.Scrollbar(frame, orient=tk.VERTICAL)\n",
    "suggestion_list = tk.Listbox(frame, font=(\"Arial\", 14), width=50, height=6, yscrollcommand=scrollbar.set)\n",
    "scrollbar.config(command=suggestion_list.yview)\n",
    "\n",
    "suggestion_list.pack(side=tk.LEFT, fill=tk.BOTH)\n",
    "scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "\n",
    "# ========== G·∫Øn s·ª± ki·ªán ==========\n",
    "entry.bind(\"<KeyRelease>\", update_suggestions)\n",
    "entry.bind(\"<Tab>\", on_tab)\n",
    "entry.bind(\"<Up>\", on_arrow_key)\n",
    "entry.bind(\"<Down>\", on_arrow_key)\n",
    "suggestion_list.bind(\"<Double-Button-1>\", on_click_suggestion)\n",
    "\n",
    "# Ch·∫°y giao di·ªán\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ======== NLP x·ª≠ l√Ω =========\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def build_ngram_model(tokens, n):\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return Counter(n_grams)\n",
    "\n",
    "def predict_next_word(model, prefix):\n",
    "    prefix = tuple(prefix)\n",
    "    candidates = {k[-1]: v for k, v in model.items() if k[:-1] == prefix}\n",
    "    return sorted(candidates.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# ======== S·ª± ki·ªán =========\n",
    "def update_suggestions(event=None):\n",
    "    words = entry_var.get().strip().split()\n",
    "    if not words or len(words) < n - 1:\n",
    "        suggestion_list.delete(0, tk.END)\n",
    "        return\n",
    "\n",
    "    prefix = words[-(n - 1):]\n",
    "    suggestions = predict_next_word(ngram_model, prefix)\n",
    "\n",
    "    suggestion_list.delete(0, tk.END)\n",
    "    for word, _ in suggestions:\n",
    "        suggestion_list.insert(tk.END, word)\n",
    "\n",
    "    # M·∫∑c ƒë·ªãnh ch·ªçn t·ª´ ƒë·∫ßu\n",
    "    if suggestion_list.size() > 0:\n",
    "        suggestion_list.selection_set(0)\n",
    "        suggestion_list.activate(0)\n",
    "\n",
    "# Tab ‚Üí ch√®n t·ª´ ƒëang ch·ªçn\n",
    "def on_tab(event):\n",
    "    if suggestion_list.size() == 0:\n",
    "        return \"break\"\n",
    "\n",
    "    try:\n",
    "        index = suggestion_list.curselection()[0]\n",
    "        selected_word = suggestion_list.get(index)\n",
    "    except IndexError:\n",
    "        return \"break\"\n",
    "\n",
    "    current_text = entry_var.get().strip()\n",
    "    new_text = f\"{current_text} {selected_word}\"\n",
    "    entry_var.set(new_text)\n",
    "    entry.icursor(tk.END)\n",
    "    suggestion_list.delete(0, tk.END)\n",
    "    return \"break\"\n",
    "\n",
    "# ‚Üë ‚Üì ‚Üí di chuy·ªÉn ƒë√∫ng 1 d√≤ng\n",
    "def on_arrow_key(event):\n",
    "    if suggestion_list.size() == 0:\n",
    "        return \"break\"\n",
    "\n",
    "    try:\n",
    "        index = suggestion_list.curselection()[0]\n",
    "    except IndexError:\n",
    "        index = -1\n",
    "\n",
    "    if event.keysym == \"Down\" and index < suggestion_list.size() - 1:\n",
    "        index += 1\n",
    "    elif event.keysym == \"Up\" and index > 0:\n",
    "        index -= 1\n",
    "    else:\n",
    "        return \"break\"\n",
    "\n",
    "    suggestion_list.selection_clear(0, tk.END)\n",
    "    suggestion_list.selection_set(index)\n",
    "    suggestion_list.activate(index)\n",
    "    suggestion_list.see(index)\n",
    "    return \"break\"\n",
    "\n",
    "# Nh·∫•n chu·ªôt ‚Üí ch·ªçn t·ª´\n",
    "def on_click_suggestion(event):\n",
    "    try:\n",
    "        index = suggestion_list.curselection()[0]\n",
    "        selected_word = suggestion_list.get(index)\n",
    "        current_text = entry_var.get().strip()\n",
    "        new_text = f\"{current_text} {selected_word}\"\n",
    "        entry_var.set(new_text)\n",
    "        entry.icursor(tk.END)\n",
    "        suggestion_list.delete(0, tk.END)\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "# ======== D·ªØ li·ªáu & m√¥ h√¨nh =========\n",
    "with open(\"data_ctut.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "tokens = preprocess_text(text)\n",
    "n = 2\n",
    "ngram_model = build_ngram_model(tokens, n)\n",
    "\n",
    "# ======== Giao di·ªán =========\n",
    "root = tk.Tk()\n",
    "root.title(\"G·ª£i √Ω t·ª´ th√¥ng minh\")\n",
    "root.geometry(\"600x300\")\n",
    "\n",
    "entry_var = tk.StringVar()\n",
    "entry = tk.Entry(root, textvariable=entry_var, font=(\"Arial\", 14), width=50)\n",
    "entry.pack(pady=10)\n",
    "\n",
    "frame = tk.Frame(root)\n",
    "frame.pack()\n",
    "\n",
    "scrollbar = tk.Scrollbar(frame, orient=tk.VERTICAL)\n",
    "suggestion_list = tk.Listbox(frame, font=(\"Arial\", 14), width=50, height=6, yscrollcommand=scrollbar.set)\n",
    "scrollbar.config(command=suggestion_list.yview)\n",
    "suggestion_list.pack(side=tk.LEFT, fill=tk.BOTH)\n",
    "scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "\n",
    "# ======== G·∫Øn s·ª± ki·ªán =========\n",
    "entry.bind(\"<KeyRelease>\", update_suggestions)\n",
    "entry.bind(\"<Tab>\", on_tab)\n",
    "entry.bind(\"<Up>\", on_arrow_key)\n",
    "entry.bind(\"<Down>\", on_arrow_key)\n",
    "suggestion_list.bind(\"<Double-Button-1>\", on_click_suggestion)\n",
    "\n",
    "# Ch·∫°y ·ª©ng d·ª•ng\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
