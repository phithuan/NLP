{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b647b0",
   "metadata": {},
   "source": [
    "## TEXT CLASSIFICATION\n",
    "\n",
    "Họ tên và MSSV: Huỳnh Chí Phi Thuận, KHDL2211038\n",
    "\n",
    "Lớp: khdl2211 Ngày học: 4/10/2025 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "196c9a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Error loading wordent: Package 'wordent' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordent')\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_word = set(stopwords.words('english'))\n",
    "\n",
    "stop_word = {'ourselves', 'hers', 'between', 'yourself', 'but', 'again',\n",
    "'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they',\n",
    "'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into',\n",
    "'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as'\n",
    "'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we',\n",
    "'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more'\n",
    "'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above',\n",
    "'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any',\n",
    "'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does'\n",
    "'yourselves', 'then', 'that', 'because', 'what', 'over', 'why',\n",
    "'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself',\n",
    "'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those',\n",
    "'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my',\n",
    "'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was',\n",
    "'here', 'than'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "636d52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(directory):\n",
    "    result = []  # Tạo một danh sách rỗng để chứa nội dung của các file\n",
    "\n",
    "    # Duyệt qua từng tên file trong thư mục được truyền vào\n",
    "    for fname in os.listdir(directory):\n",
    "\n",
    "        # Mở từng file ở chế độ đọc, dùng mã hóa ISO-8859-1 (hợp với dữ liệu Enron)\n",
    "        with open(directory + '/' + fname, 'r', encoding='ISO-8859-1') as f:\n",
    "            result.append(f.read())  # Đọc toàn bộ nội dung file và thêm vào danh sách\n",
    "\n",
    "    return result  # Trả về danh sách chứa nội dung của tất cả các file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f0748ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # Khởi tạo bộ lemmatizer của NLTK để đưa từ về gốc (run, runs → run)\n",
    "    lematizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    # Tách câu thành từng từ (tokens)\n",
    "    processed_tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Chuyển tất cả từ thành chữ thường để chuẩn hóa (Avoid → avoid)\n",
    "    processed_tokens = [w.lower() for w in processed_tokens]\n",
    "\n",
    "    # Đếm số lần xuất hiện của từng từ\n",
    "    word_counts = collections.Counter(processed_tokens)\n",
    "\n",
    "    # Lấy ra những từ xuất hiện ít nhất (uncommon words)\n",
    "    # most_common()[:-10:-1] = 9 từ ít phổ biến nhất (nhưng cách viết này khá mơ hồ)\n",
    "    uncommon_words = word_counts.most_common()[:-10:-1]\n",
    "\n",
    "    # Bước lọc 1: Loại bỏ stop words (các từ phổ biến như: the, is, and, ...)\n",
    "    processed_tokens = [w for w in processed_tokens if w not in stop_word]\n",
    "\n",
    "    # Bước lọc 2: Loại bỏ từ ít xuất hiện (uncommon words)\n",
    "    processed_tokens = [w for w in processed_tokens if w not in uncommon_words]\n",
    "\n",
    "    # Lemmatize: đưa từ về gốc ngữ nghĩa (better → good, running → run)\n",
    "    processed_tokens = [lematizer.lemmatize(w) for w in processed_tokens]\n",
    "\n",
    "    # Trả về danh sách token đã xử lý\n",
    "    return processed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc6a5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(tokens):\n",
    "    '''\n",
    "    Mô tả:\n",
    "    - Hàm này chuyển mỗi từ (token) thành một đặc trưng (feature).\n",
    "    - Giá trị của đặc trưng là số lần từ đó xuất hiện trong câu.\n",
    "    - Kết quả trả về là một dictionary dạng: {'word1': count1, 'word2': count2, ...}\n",
    "    '''\n",
    "\n",
    "    # collections.Counter sẽ đếm số lần xuất hiện của mỗi từ trong danh sách tokens\n",
    "    # dict(...) chuyển kết quả Counter thành dictionary bình thường\n",
    "    return dict(collections.Counter(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "194ca3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, train_size=0.8):\n",
    "    # Tính số lượng mẫu dùng cho tập huấn luyện dựa vào tỷ lệ train_size\n",
    "    num_training_examples = int(len(dataset) * train_size)\n",
    "\n",
    "    # Trả về hai phần: \n",
    "    # - phần đầu là tập huấn luyện (training set)\n",
    "    # - phần sau là tập kiểm tra (test set)\n",
    "    return dataset[:num_training_examples], dataset[num_training_examples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61ea738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18080\\1026088342.py:9: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  positive_examples = load_file('data\\ham')\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18080\\1026088342.py:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  negative_examples = load_file('data\\spam')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5172 emails processed.\n",
      "✅ Accuracy on training set: 96.10\n",
      "✅ Accuracy on test set: 93.88\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import collections\n",
    "\n",
    "# ===== 1. Đọc dữ liệu từ thư mục =====\n",
    "# positive: spam, label = 1\n",
    "# negative: ham, label = 0\n",
    "\n",
    "positive_examples = load_file('data\\ham')\n",
    "negative_examples = load_file('data\\spam')\n",
    "\n",
    "# ===== 2. Tiền xử lý mỗi email =====\n",
    "# Biến danh sách chuỗi thành danh sách token sau khi xử lý (lower, stopword, lemmatize,...)\n",
    "positive_examples = [preprocess_sentence(email) for email in positive_examples]\n",
    "negative_examples = [preprocess_sentence(email) for email in negative_examples]\n",
    "\n",
    "# ===== 3. Gắn nhãn (label) cho từng email =====\n",
    "positive_examples = [(email, 1) for email in positive_examples]  # spam = 1\n",
    "negative_examples = [(email, 0) for email in negative_examples]  # ham = 0\n",
    "\n",
    "# ===== 4. Gộp và xáo trộn toàn bộ dữ liệu =====\n",
    "all_examples = positive_examples + negative_examples\n",
    "random.shuffle(all_examples)\n",
    "\n",
    "# In ra số lượng email đã xử lý\n",
    "print('{} emails processed.'.format(len(all_examples)))\n",
    "\n",
    "# ===== 5. Trích xuất đặc trưng từ mỗi email =====\n",
    "# Tạo bộ dữ liệu gồm (features, label) theo định dạng của NLTK NaiveBayesClassifier\n",
    "featurized = [(feature_extraction(email), label) for email, label in all_examples]\n",
    "\n",
    "# ===== 6. Chia dữ liệu train/test =====\n",
    "training_set, test_set = train_test_split(featurized, train_size=0.7)\n",
    "\n",
    "# ===== 7. Huấn luyện mô hình Naive Bayes (NLTK) =====\n",
    "model = nltk.classify.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# ===== 8. Đánh giá trên tập huấn luyện =====\n",
    "training_error = nltk.classify.accuracy(model, training_set)\n",
    "print('✅ Accuracy on training set: {:.2f}'.format(training_error * 100))\n",
    "\n",
    "# ===== 9. Đánh giá trên tập kiểm tra =====\n",
    "testing_error = nltk.classify.accuracy(model, test_set)\n",
    "print('✅ Accuracy on test set: {:.2f}'.format(testing_error * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b53877",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2d7ff53",
   "metadata": {},
   "source": [
    "Dưới đây là phần trả lời chi tiết cho từng câu hỏi của bạn liên quan đến tiền xử lý văn bản và mô hình phân loại (ví dụ như email spam):\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **1. Mục đích của việc sử dụng `WordNetLemmatizer` trong tiền xử lý văn bản là gì?**\n",
    "\n",
    "**WordNetLemmatizer** giúp đưa một từ về **dạng gốc (lemma)** của nó, ví dụ:\n",
    "- \"running\" → \"run\"\n",
    "- \"better\" → \"good\"\n",
    "\n",
    "👉 Mục đích:\n",
    "- **Giảm số lượng từ khác nhau nhưng cùng gốc**, giúp mô hình học hiệu quả hơn.\n",
    "- Tăng **tính tổng quát** và **giảm nhiễu** trong tập dữ liệu.\n",
    "- Cải thiện độ chính xác khi trích xuất đặc trưng.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **2. Trong đoạn mã `train_test_split`, tham số `train_size=0.8` có ý nghĩa gì?**\n",
    "\n",
    "`train_size=0.8` có nghĩa là:\n",
    "- 80% dữ liệu sẽ được dùng để **huấn luyện mô hình**.\n",
    "- 20% còn lại dùng để **kiểm tra và đánh giá** mô hình.\n",
    "\n",
    "👉 Đây là tỷ lệ phổ biến để đảm bảo mô hình học tốt mà vẫn được kiểm tra khách quan.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **3. Tại sao chúng ta cần xáo trộn dữ liệu trước khi chia tập huấn luyện và kiểm tra?**\n",
    "\n",
    "Nếu dữ liệu được sắp xếp theo thứ tự (ví dụ: tất cả email spam ở đầu, không spam ở cuối), thì:\n",
    "- **Tập huấn luyện** có thể chỉ chứa một loại dữ liệu → mô hình học không đủ thông tin.\n",
    "- Dễ bị **overfitting** hoặc **underfitting**.\n",
    "\n",
    "👉 Việc **xáo trộn (shuffle)** đảm bảo:\n",
    "- Hai tập (train/test) đều đại diện đầy đủ cho các loại dữ liệu khác nhau.\n",
    "- Mô hình **tổng quát tốt hơn** khi áp dụng vào dữ liệu thực tế.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **4. Mô hình sử dụng thuật toán nào để phân loại email spam?**\n",
    "\n",
    "Thông thường trong các ví dụ đơn giản, mô hình phân loại email spam sử dụng thuật toán:\n",
    "🔸 **Naive Bayes (Multinomial Naive Bayes)**\n",
    "\n",
    "- Đây là thuật toán thường dùng cho **phân loại văn bản** do:\n",
    "  - Hiệu quả cao\n",
    "  - Tính toán nhanh\n",
    "  - Phù hợp với dữ liệu biểu diễn dưới dạng tần suất từ\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **5. Nếu muốn cải thiện độ chính xác của mô hình, bạn có thể thực hiện những thay đổi nào?**\n",
    "\n",
    "Một số cách phổ biến:\n",
    "- **Tiền xử lý nâng cao**: loại bỏ stop words, lemmatization/stemming, chuẩn hóa văn bản.\n",
    "- **Tối ưu vector hóa**:\n",
    "  - Dùng TF-IDF thay vì CountVectorizer.\n",
    "  - Sử dụng embedding (Word2Vec, BERT) nếu cần độ chính xác cao.\n",
    "- **Tăng dữ liệu huấn luyện**.\n",
    "- **Cân bằng dữ liệu** nếu bị lệch nhãn (spam/ham không đồng đều).\n",
    "- **Tối ưu mô hình**:\n",
    "  - Thử các thuật toán khác: SVM, Random Forest, XGBoost.\n",
    "  - Điều chỉnh siêu tham số (hyperparameter tuning).\n",
    "- **Loại bỏ nhiễu/ngôn ngữ không cần thiết** trong dữ liệu.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **6. Vì sao Naive Bayes lại phù hợp với bài toán phân loại văn bản?**\n",
    "\n",
    "🔹 Naive Bayes phù hợp vì:\n",
    "- **Giả định độc lập** giữa các đặc trưng (từ ngữ) — đơn giản nhưng hiệu quả trong thực tế.\n",
    "- **Hiệu suất nhanh**, phù hợp với dữ liệu văn bản lớn.\n",
    "- **Ít tài nguyên tính toán**.\n",
    "- Đặc biệt hiệu quả khi đặc trưng là tần suất từ hoặc TF-IDF.\n",
    "\n",
    "👉 Mặc dù giả định độc lập là đơn giản hoá, nhưng vẫn cho kết quả rất tốt trong các bài toán như:\n",
    "- **Phân loại email spam**\n",
    "- **Phân loại tin tức, đánh giá sentiment (cảm xúc)**\n",
    "\n",
    "---\n",
    "\n",
    "Nếu bạn đang làm bài tập hoặc dự án cụ thể, mình có thể giúp mở rộng từng phần theo hướng đó luôn nhé!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a0698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
