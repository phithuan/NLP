{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b647b0",
   "metadata": {},
   "source": [
    "## TEXT CLASSIFICATION\n",
    "\n",
    "H·ªç t√™n v√† MSSV: Hu·ª≥nh Ch√≠ Phi Thu·∫≠n, KHDL2211038\n",
    "\n",
    "L·ªõp: khdl2211 Ng√†y h·ªçc: 4/10/2025 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "196c9a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Error loading wordent: Package 'wordent' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordent')\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_word = set(stopwords.words('english'))\n",
    "\n",
    "stop_word = {'ourselves', 'hers', 'between', 'yourself', 'but', 'again',\n",
    "'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they',\n",
    "'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into',\n",
    "'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as'\n",
    "'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we',\n",
    "'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more'\n",
    "'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above',\n",
    "'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any',\n",
    "'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does'\n",
    "'yourselves', 'then', 'that', 'because', 'what', 'over', 'why',\n",
    "'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself',\n",
    "'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those',\n",
    "'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my',\n",
    "'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was',\n",
    "'here', 'than'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "636d52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(directory):\n",
    "    result = []  # T·∫°o m·ªôt danh s√°ch r·ªóng ƒë·ªÉ ch·ª©a n·ªôi dung c·ªßa c√°c file\n",
    "\n",
    "    # Duy·ªát qua t·ª´ng t√™n file trong th∆∞ m·ª•c ƒë∆∞·ª£c truy·ªÅn v√†o\n",
    "    for fname in os.listdir(directory):\n",
    "\n",
    "        # M·ªü t·ª´ng file ·ªü ch·∫ø ƒë·ªô ƒë·ªçc, d√πng m√£ h√≥a ISO-8859-1 (h·ª£p v·ªõi d·ªØ li·ªáu Enron)\n",
    "        with open(directory + '/' + fname, 'r', encoding='ISO-8859-1') as f:\n",
    "            result.append(f.read())  # ƒê·ªçc to√†n b·ªô n·ªôi dung file v√† th√™m v√†o danh s√°ch\n",
    "\n",
    "    return result  # Tr·∫£ v·ªÅ danh s√°ch ch·ª©a n·ªôi dung c·ªßa t·∫•t c·∫£ c√°c file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f0748ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # Kh·ªüi t·∫°o b·ªô lemmatizer c·ªßa NLTK ƒë·ªÉ ƒë∆∞a t·ª´ v·ªÅ g·ªëc (run, runs ‚Üí run)\n",
    "    lematizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    # T√°ch c√¢u th√†nh t·ª´ng t·ª´ (tokens)\n",
    "    processed_tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Chuy·ªÉn t·∫•t c·∫£ t·ª´ th√†nh ch·ªØ th∆∞·ªùng ƒë·ªÉ chu·∫©n h√≥a (Avoid ‚Üí avoid)\n",
    "    processed_tokens = [w.lower() for w in processed_tokens]\n",
    "\n",
    "    # ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´ng t·ª´\n",
    "    word_counts = collections.Counter(processed_tokens)\n",
    "\n",
    "    # L·∫•y ra nh·ªØng t·ª´ xu·∫•t hi·ªán √≠t nh·∫•t (uncommon words)\n",
    "    # most_common()[:-10:-1] = 9 t·ª´ √≠t ph·ªï bi·∫øn nh·∫•t (nh∆∞ng c√°ch vi·∫øt n√†y kh√° m∆° h·ªì)\n",
    "    uncommon_words = word_counts.most_common()[:-10:-1]\n",
    "\n",
    "    # B∆∞·ªõc l·ªçc 1: Lo·∫°i b·ªè stop words (c√°c t·ª´ ph·ªï bi·∫øn nh∆∞: the, is, and, ...)\n",
    "    processed_tokens = [w for w in processed_tokens if w not in stop_word]\n",
    "\n",
    "    # B∆∞·ªõc l·ªçc 2: Lo·∫°i b·ªè t·ª´ √≠t xu·∫•t hi·ªán (uncommon words)\n",
    "    processed_tokens = [w for w in processed_tokens if w not in uncommon_words]\n",
    "\n",
    "    # Lemmatize: ƒë∆∞a t·ª´ v·ªÅ g·ªëc ng·ªØ nghƒ©a (better ‚Üí good, running ‚Üí run)\n",
    "    processed_tokens = [lematizer.lemmatize(w) for w in processed_tokens]\n",
    "\n",
    "    # Tr·∫£ v·ªÅ danh s√°ch token ƒë√£ x·ª≠ l√Ω\n",
    "    return processed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc6a5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(tokens):\n",
    "    '''\n",
    "    M√¥ t·∫£:\n",
    "    - H√†m n√†y chuy·ªÉn m·ªói t·ª´ (token) th√†nh m·ªôt ƒë·∫∑c tr∆∞ng (feature).\n",
    "    - Gi√° tr·ªã c·ªßa ƒë·∫∑c tr∆∞ng l√† s·ªë l·∫ßn t·ª´ ƒë√≥ xu·∫•t hi·ªán trong c√¢u.\n",
    "    - K·∫øt qu·∫£ tr·∫£ v·ªÅ l√† m·ªôt dictionary d·∫°ng: {'word1': count1, 'word2': count2, ...}\n",
    "    '''\n",
    "\n",
    "    # collections.Counter s·∫Ω ƒë·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªói t·ª´ trong danh s√°ch tokens\n",
    "    # dict(...) chuy·ªÉn k·∫øt qu·∫£ Counter th√†nh dictionary b√¨nh th∆∞·ªùng\n",
    "    return dict(collections.Counter(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "194ca3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, train_size=0.8):\n",
    "    # T√≠nh s·ªë l∆∞·ª£ng m·∫´u d√πng cho t·∫≠p hu·∫•n luy·ªán d·ª±a v√†o t·ª∑ l·ªá train_size\n",
    "    num_training_examples = int(len(dataset) * train_size)\n",
    "\n",
    "    # Tr·∫£ v·ªÅ hai ph·∫ßn: \n",
    "    # - ph·∫ßn ƒë·∫ßu l√† t·∫≠p hu·∫•n luy·ªán (training set)\n",
    "    # - ph·∫ßn sau l√† t·∫≠p ki·ªÉm tra (test set)\n",
    "    return dataset[:num_training_examples], dataset[num_training_examples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61ea738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18080\\1026088342.py:9: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  positive_examples = load_file('data\\ham')\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18080\\1026088342.py:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  negative_examples = load_file('data\\spam')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5172 emails processed.\n",
      "‚úÖ Accuracy on training set: 96.10\n",
      "‚úÖ Accuracy on test set: 93.88\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import collections\n",
    "\n",
    "# ===== 1. ƒê·ªçc d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c =====\n",
    "# positive: spam, label = 1\n",
    "# negative: ham, label = 0\n",
    "\n",
    "positive_examples = load_file('data\\ham')\n",
    "negative_examples = load_file('data\\spam')\n",
    "\n",
    "# ===== 2. Ti·ªÅn x·ª≠ l√Ω m·ªói email =====\n",
    "# Bi·∫øn danh s√°ch chu·ªói th√†nh danh s√°ch token sau khi x·ª≠ l√Ω (lower, stopword, lemmatize,...)\n",
    "positive_examples = [preprocess_sentence(email) for email in positive_examples]\n",
    "negative_examples = [preprocess_sentence(email) for email in negative_examples]\n",
    "\n",
    "# ===== 3. G·∫Øn nh√£n (label) cho t·ª´ng email =====\n",
    "positive_examples = [(email, 1) for email in positive_examples]  # spam = 1\n",
    "negative_examples = [(email, 0) for email in negative_examples]  # ham = 0\n",
    "\n",
    "# ===== 4. G·ªôp v√† x√°o tr·ªôn to√†n b·ªô d·ªØ li·ªáu =====\n",
    "all_examples = positive_examples + negative_examples\n",
    "random.shuffle(all_examples)\n",
    "\n",
    "# In ra s·ªë l∆∞·ª£ng email ƒë√£ x·ª≠ l√Ω\n",
    "print('{} emails processed.'.format(len(all_examples)))\n",
    "\n",
    "# ===== 5. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ m·ªói email =====\n",
    "# T·∫°o b·ªô d·ªØ li·ªáu g·ªìm (features, label) theo ƒë·ªãnh d·∫°ng c·ªßa NLTK NaiveBayesClassifier\n",
    "featurized = [(feature_extraction(email), label) for email, label in all_examples]\n",
    "\n",
    "# ===== 6. Chia d·ªØ li·ªáu train/test =====\n",
    "training_set, test_set = train_test_split(featurized, train_size=0.7)\n",
    "\n",
    "# ===== 7. Hu·∫•n luy·ªán m√¥ h√¨nh Naive Bayes (NLTK) =====\n",
    "model = nltk.classify.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# ===== 8. ƒê√°nh gi√° tr√™n t·∫≠p hu·∫•n luy·ªán =====\n",
    "training_error = nltk.classify.accuracy(model, training_set)\n",
    "print('‚úÖ Accuracy on training set: {:.2f}'.format(training_error * 100))\n",
    "\n",
    "# ===== 9. ƒê√°nh gi√° tr√™n t·∫≠p ki·ªÉm tra =====\n",
    "testing_error = nltk.classify.accuracy(model, test_set)\n",
    "print('‚úÖ Accuracy on test set: {:.2f}'.format(testing_error * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b53877",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2d7ff53",
   "metadata": {},
   "source": [
    "D∆∞·ªõi ƒë√¢y l√† ph·∫ßn tr·∫£ l·ªùi chi ti·∫øt cho t·ª´ng c√¢u h·ªèi c·ªßa b·∫°n li√™n quan ƒë·∫øn ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n v√† m√¥ h√¨nh ph√¢n lo·∫°i (v√≠ d·ª• nh∆∞ email spam):\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **1. M·ª•c ƒë√≠ch c·ªßa vi·ªác s·ª≠ d·ª•ng `WordNetLemmatizer` trong ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n l√† g√¨?**\n",
    "\n",
    "**WordNetLemmatizer** gi√∫p ƒë∆∞a m·ªôt t·ª´ v·ªÅ **d·∫°ng g·ªëc (lemma)** c·ªßa n√≥, v√≠ d·ª•:\n",
    "- \"running\" ‚Üí \"run\"\n",
    "- \"better\" ‚Üí \"good\"\n",
    "\n",
    "üëâ M·ª•c ƒë√≠ch:\n",
    "- **Gi·∫£m s·ªë l∆∞·ª£ng t·ª´ kh√°c nhau nh∆∞ng c√πng g·ªëc**, gi√∫p m√¥ h√¨nh h·ªçc hi·ªáu qu·∫£ h∆°n.\n",
    "- TƒÉng **t√≠nh t·ªïng qu√°t** v√† **gi·∫£m nhi·ªÖu** trong t·∫≠p d·ªØ li·ªáu.\n",
    "- C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c khi tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **2. Trong ƒëo·∫°n m√£ `train_test_split`, tham s·ªë `train_size=0.8` c√≥ √Ω nghƒ©a g√¨?**\n",
    "\n",
    "`train_size=0.8` c√≥ nghƒ©a l√†:\n",
    "- 80% d·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c d√πng ƒë·ªÉ **hu·∫•n luy·ªán m√¥ h√¨nh**.\n",
    "- 20% c√≤n l·∫°i d√πng ƒë·ªÉ **ki·ªÉm tra v√† ƒë√°nh gi√°** m√¥ h√¨nh.\n",
    "\n",
    "üëâ ƒê√¢y l√† t·ª∑ l·ªá ph·ªï bi·∫øn ƒë·ªÉ ƒë·∫£m b·∫£o m√¥ h√¨nh h·ªçc t·ªët m√† v·∫´n ƒë∆∞·ª£c ki·ªÉm tra kh√°ch quan.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **3. T·∫°i sao ch√∫ng ta c·∫ßn x√°o tr·ªôn d·ªØ li·ªáu tr∆∞·ªõc khi chia t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra?**\n",
    "\n",
    "N·∫øu d·ªØ li·ªáu ƒë∆∞·ª£c s·∫Øp x·∫øp theo th·ª© t·ª± (v√≠ d·ª•: t·∫•t c·∫£ email spam ·ªü ƒë·∫ßu, kh√¥ng spam ·ªü cu·ªëi), th√¨:\n",
    "- **T·∫≠p hu·∫•n luy·ªán** c√≥ th·ªÉ ch·ªâ ch·ª©a m·ªôt lo·∫°i d·ªØ li·ªáu ‚Üí m√¥ h√¨nh h·ªçc kh√¥ng ƒë·ªß th√¥ng tin.\n",
    "- D·ªÖ b·ªã **overfitting** ho·∫∑c **underfitting**.\n",
    "\n",
    "üëâ Vi·ªác **x√°o tr·ªôn (shuffle)** ƒë·∫£m b·∫£o:\n",
    "- Hai t·∫≠p (train/test) ƒë·ªÅu ƒë·∫°i di·ªán ƒë·∫ßy ƒë·ªß cho c√°c lo·∫°i d·ªØ li·ªáu kh√°c nhau.\n",
    "- M√¥ h√¨nh **t·ªïng qu√°t t·ªët h∆°n** khi √°p d·ª•ng v√†o d·ªØ li·ªáu th·ª±c t·∫ø.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **4. M√¥ h√¨nh s·ª≠ d·ª•ng thu·∫≠t to√°n n√†o ƒë·ªÉ ph√¢n lo·∫°i email spam?**\n",
    "\n",
    "Th√¥ng th∆∞·ªùng trong c√°c v√≠ d·ª• ƒë∆°n gi·∫£n, m√¥ h√¨nh ph√¢n lo·∫°i email spam s·ª≠ d·ª•ng thu·∫≠t to√°n:\n",
    "üî∏ **Naive Bayes (Multinomial Naive Bayes)**\n",
    "\n",
    "- ƒê√¢y l√† thu·∫≠t to√°n th∆∞·ªùng d√πng cho **ph√¢n lo·∫°i vƒÉn b·∫£n** do:\n",
    "  - Hi·ªáu qu·∫£ cao\n",
    "  - T√≠nh to√°n nhanh\n",
    "  - Ph√π h·ª£p v·ªõi d·ªØ li·ªáu bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng t·∫ßn su·∫•t t·ª´\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **5. N·∫øu mu·ªën c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh, b·∫°n c√≥ th·ªÉ th·ª±c hi·ªán nh·ªØng thay ƒë·ªïi n√†o?**\n",
    "\n",
    "M·ªôt s·ªë c√°ch ph·ªï bi·∫øn:\n",
    "- **Ti·ªÅn x·ª≠ l√Ω n√¢ng cao**: lo·∫°i b·ªè stop words, lemmatization/stemming, chu·∫©n h√≥a vƒÉn b·∫£n.\n",
    "- **T·ªëi ∆∞u vector h√≥a**:\n",
    "  - D√πng TF-IDF thay v√¨ CountVectorizer.\n",
    "  - S·ª≠ d·ª•ng embedding (Word2Vec, BERT) n·∫øu c·∫ßn ƒë·ªô ch√≠nh x√°c cao.\n",
    "- **TƒÉng d·ªØ li·ªáu hu·∫•n luy·ªán**.\n",
    "- **C√¢n b·∫±ng d·ªØ li·ªáu** n·∫øu b·ªã l·ªách nh√£n (spam/ham kh√¥ng ƒë·ªìng ƒë·ªÅu).\n",
    "- **T·ªëi ∆∞u m√¥ h√¨nh**:\n",
    "  - Th·ª≠ c√°c thu·∫≠t to√°n kh√°c: SVM, Random Forest, XGBoost.\n",
    "  - ƒêi·ªÅu ch·ªânh si√™u tham s·ªë (hyperparameter tuning).\n",
    "- **Lo·∫°i b·ªè nhi·ªÖu/ng√¥n ng·ªØ kh√¥ng c·∫ßn thi·∫øt** trong d·ªØ li·ªáu.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **6. V√¨ sao Naive Bayes l·∫°i ph√π h·ª£p v·ªõi b√†i to√°n ph√¢n lo·∫°i vƒÉn b·∫£n?**\n",
    "\n",
    "üîπ Naive Bayes ph√π h·ª£p v√¨:\n",
    "- **Gi·∫£ ƒë·ªãnh ƒë·ªôc l·∫≠p** gi·ªØa c√°c ƒë·∫∑c tr∆∞ng (t·ª´ ng·ªØ) ‚Äî ƒë∆°n gi·∫£n nh∆∞ng hi·ªáu qu·∫£ trong th·ª±c t·∫ø.\n",
    "- **Hi·ªáu su·∫•t nhanh**, ph√π h·ª£p v·ªõi d·ªØ li·ªáu vƒÉn b·∫£n l·ªõn.\n",
    "- **√çt t√†i nguy√™n t√≠nh to√°n**.\n",
    "- ƒê·∫∑c bi·ªát hi·ªáu qu·∫£ khi ƒë·∫∑c tr∆∞ng l√† t·∫ßn su·∫•t t·ª´ ho·∫∑c TF-IDF.\n",
    "\n",
    "üëâ M·∫∑c d√π gi·∫£ ƒë·ªãnh ƒë·ªôc l·∫≠p l√† ƒë∆°n gi·∫£n ho√°, nh∆∞ng v·∫´n cho k·∫øt qu·∫£ r·∫•t t·ªët trong c√°c b√†i to√°n nh∆∞:\n",
    "- **Ph√¢n lo·∫°i email spam**\n",
    "- **Ph√¢n lo·∫°i tin t·ª©c, ƒë√°nh gi√° sentiment (c·∫£m x√∫c)**\n",
    "\n",
    "---\n",
    "\n",
    "N·∫øu b·∫°n ƒëang l√†m b√†i t·∫≠p ho·∫∑c d·ª± √°n c·ª• th·ªÉ, m√¨nh c√≥ th·ªÉ gi√∫p m·ªü r·ªông t·ª´ng ph·∫ßn theo h∆∞·ªõng ƒë√≥ lu√¥n nh√©!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a0698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
