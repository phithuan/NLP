{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TH·ª∞C H√ÄNH BU·ªîI 2\n",
    "\n",
    "## TI·ªÄN X·ª¨ L√ù VƒÇN B·∫¢N\n",
    "\n",
    "H·ªç t√™n v√† MSSV: Hu·ª≥nh Ch√≠ Phi Thu·∫≠n\n",
    "\n",
    "L·ªõp: Ng√†y h·ªçc: KHDL2211 ng√†y 27 th√°ng 2 nƒÉm 2025\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Th·ª±c hi·ªán c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω c∆° b·∫£n\n",
    "\n",
    "## Sinh vi√™n th·ª±c hi·ªán c√°c ƒëo·∫°n code trong file Lap2_NLP_Code v√† cho bi·∫øt:\n",
    "- a) K·∫øt qu·∫£.\n",
    "- b) T·ª´ng ƒëo·∫°n code ƒëang th·ª±c hi·ªán b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω n√†o. Gi·∫£i th√≠ch qu√° tr√¨nh x·ª≠ l√Ω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nüìå nltk.download('punkt') gi√∫p t√°ch c√¢u, t√°ch t·ª´.\\nüìå nltk.download('wordnet') h·ªó tr·ª£ tra c·ª©u t·ª´ ƒëi·ªÉn.\\nüìå sent_tokenize() & word_tokenize() d√πng ƒë·ªÉ chia nh·ªè vƒÉn b·∫£n.\\nüìå wordnet gi√∫p x·ª≠ l√Ω t·ª´ v·ª±ng nh∆∞ tra nghƒ©a, t·ª´ ƒë·ªìng nghƒ©a, tr√°i nghƒ©a\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')  # C·∫ßn cho sent_tokenize v√† word_tokenize\n",
    "nltk.download('wordnet')  # C·∫ßn cho wordnet\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\"\"\"\n",
    "üìå nltk.download('punkt') gi√∫p t√°ch c√¢u, t√°ch t·ª´.\n",
    "üìå nltk.download('wordnet') h·ªó tr·ª£ tra c·ª©u t·ª´ ƒëi·ªÉn.\n",
    "üìå sent_tokenize() & word_tokenize() d√πng ƒë·ªÉ chia nh·ªè vƒÉn b·∫£n.\n",
    "üìå wordnet gi√∫p x·ª≠ l√Ω t·ª´ v·ª±ng nh∆∞ tra nghƒ©a, t·ª´ ƒë·ªìng nghƒ©a, tr√°i nghƒ©a\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE 1\n",
    "\n",
    "Ch·ª©c nƒÉng\tM√¥ t·∫£\n",
    "\n",
    "sent_tokenize(my_text)\tChia vƒÉn b·∫£n th√†nh c√¢u\n",
    "\n",
    "word_tokenize(my_text)\tChia vƒÉn b·∫£n th√†nh t·ª´\n",
    "\n",
    "list(my_text)\tChuy·ªÉn vƒÉn b·∫£n th√†nh danh s√°ch k√Ω t·ª±\n",
    "\n",
    "wordnet.synsets(\"NLP\")\tTra c·ª©u nghƒ©a c·ªßa t·ª´ \"NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there.', 'Welcome to new semester.', 'How are you?', 'Today is a good.']\n",
      "1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-\n",
      "['Hello', 'there', '.', 'Welcome', 'to', 'new', 'semester', '.', 'How', 'are', 'you', '?', 'Today', 'is', 'a', 'good', '.']\n",
      "--------------------------------------------------\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 't', 'h', 'e', 'r', 'e', '.', ' ', 'W', 'e', 'l', 'c', 'o', 'm', 'e', ' ', 't', 'o', ' ', 'n', 'e', 'w', ' ', 's', 'e', 'm', 'e', 's', 't', 'e', 'r', '.', ' ', 'H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?', ' ', 'T', 'o', 'd', 'a', 'y', ' ', 'i', 's', ' ', 'a', ' ', 'g', 'o', 'o', 'd', '.']\n",
      "--------------------------------------------------\n",
      "the branch of information science that deals with natural language information\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  # S·ª≠a l·ªói ·ªü ƒë√¢y\n",
    "    my_text = \"Hello there. Welcome to new semester. How are you? Today is a good.\"\n",
    "\n",
    "    print(sent_tokenize(my_text)) # n√†y ƒë∆∞·ª£c t√°ch ra theo t·ª´ng d·∫•u c√¢u sau n√≥ nh∆∞ !  . ? √°. theo t·ª´ng c√¢u ri√™ng bi·ªát\n",
    "    print(\"1-\"*50) # m√†u m√® n·ªØa\n",
    "\n",
    "    print(word_tokenize(my_text)) # ƒëo·∫°n n√†y t√°ch t·ª´ng c√¢u ra lu√¥n d√¢u , ? c≈©ng t√°ch n·ªØa\n",
    "    print(\"-\"*50) # m√†u m√® n·ªØa\n",
    "\n",
    "    print(list(my_text)) # danh s√°ch m√† n√≥ k√¨ k√¨ ta. t√°ch ra t·ª´ng ch·ªØ c√°i lu√¥n\n",
    "    print(\"-\"*50) # m√†u m√® n·ªØa\n",
    "\n",
    "    syn = wordnet.synsets(\"NLP\")\n",
    "    if syn:\n",
    "        print(syn[0].definition())\n",
    "    else:\n",
    "        print(\"no definition found for NLP.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 66\n",
      "Total sentences in sample_sentences: 1\n",
      "sentence 1: We will discuss briefly about the basic syntax, structure and desi\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sample_text = \"We will discuss briefly about the basic syntax, structure and desi\"\n",
    "\n",
    "print(\"Total characters: \" + str(len(sample_text))) # s·ªë k√≠ t·ª± trong chu·ªói bao g·ªìm d·∫•u c√°ch\n",
    "\n",
    "# import sentence tokenization\n",
    "default_st = nltk.sent_tokenize\n",
    "\n",
    "# apply on the sample_text\n",
    "sample_sentences = default_st(text=sample_text) # t√°ch ra t·ª´ng c√¢u ri√™ng bi·ªát\n",
    "# print(sent_tokenize(sample_text))\n",
    "\n",
    "print(\"Total sentences in sample_sentences: \" + str(len(sample_sentences)))\n",
    "for i in range (len(sample_sentences)):\n",
    "    print(\"sentence \" + str(i+1) + \": \" + sample_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:  144395\n",
      "--------------------------------------------------\n",
      "First 100 characters in the corpus\n",
      "\n",
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n",
      "--------------------------------------------------\n",
      "Total sentences in alice:  1625\n",
      "--------------------------------------------------\n",
      "First 2 sentences in alice: \n",
      "sentence 1: [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I.\n",
      "sentence 2: Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Download the 'gutenberg' corpus #T·∫£i xu·ªëng v√† ƒë·ªçc n·ªôi dung c·ªßa vƒÉn b·∫£n Alice‚Äôs Adventures in Wonderland t·ª´ t·∫≠p d·ªØ li·ªáu gutenberg trong nltk\n",
    "#nltk.download('gutenberg')\n",
    "\n",
    "alice = gutenberg.raw(fileids=\"carroll-alice.txt\")\n",
    "print(\"Total characters: \", str(len(alice)))\n",
    "\n",
    "print(\"-\"*50) # m√†u m√® n·ªØa\n",
    "\n",
    "\n",
    "print(\"First 100 characters in the corpus\\n\")\n",
    "print(alice[0:100])\n",
    "\n",
    "print(\"-\"*50) # m√†u m√® n·ªØa\n",
    "\n",
    "## import sentence tokenization\n",
    "default_st = nltk.sent_tokenize # t√°ch ra t·ª´ng c√¢u ri√™ng bi·ªát\n",
    "\n",
    "# apply on the sample_text\n",
    "alice_sentences = default_st(text=alice) # t√°ch ra t·ª´ng c√¢u ri√™ng bi·ªát\n",
    "\n",
    "print(\"Total sentences in alice: \", str(len(alice_sentences)))\n",
    "print(\"-\"*50)\n",
    "print(\"First 2 sentences in alice: \")\n",
    "print(\"sentence 1: \" + alice_sentences[0])\n",
    "print(\"sentence 2: \" + alice_sentences[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "['The', 'brown', 'fox', 'wasn', '‚Äô', 't', 'that', 'quick', 'and', 'he', 'couldn', '‚Äô', 't', 'win', 'the', 'race', '.', 'Hey', 'that', '‚Äô', 's', 'a', 'great', 'deal', '!', 'I', 'just', 'bought', 'a', 'phone', 'for', '$', '199', '.', 'You', '‚Äô', 'll', 'learn', 'a', 'lot', 'in', 'the', 'BOOK', '.', 'Python', 'is', 'an', 'amazing', 'language', '!']\n",
      "--------------------------------------------------\n",
      "C√°ch ƒë·ªÉ gi·ªØ nguy√™n contractions (kh√¥ng t√°ch wasn't th√†nh was  n't)\n",
      "36\n",
      "['The', 'brown', 'fox', 'wasn‚Äôt', 'that', 'quick', 'and', 'he', 'couldn‚Äôt', 'win', 'the', 'race.', 'Hey', 'that‚Äôs', 'a', 'great', 'deal!', 'I', 'just', 'bought', 'a', 'phone', 'for', '$199.', 'You‚Äôll', 'learn', 'a', 'lot', 'in', 'the', 'BOOK.', 'Python', 'is', 'an', 'amazing', 'language!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sample_text = \"The brown fox wasn‚Äôt that quick and he couldn‚Äôt win the race. Hey that‚Äôs a great deal! I just bought a phone for $199. You‚Äôll learn a lot in the BOOK. Python is an amazing language!\"\n",
    "\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(text=sample_text)\n",
    "\n",
    "print(len(words)) # ƒë·∫øm s·ªë t·ª´ \n",
    "print(words) \n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"C√°ch ƒë·ªÉ gi·ªØ nguy√™n contractions (kh√¥ng t√°ch wasn't th√†nh was  n't)\")\n",
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sample_text)\n",
    "print(len(words)) # ƒë·∫øm s·ªë t·ª´ \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race', '.', 'Hey']\n",
      "--------------------\n",
      "['The', 'brown', 'fox', 'was', 'nt', 'that', 'quick', 'and', 'he', 'could', 'nt', 'win', 'the', 'race', 'Hey']\n"
     ]
    }
   ],
   "source": [
    "# code 4.1\n",
    "\"\"\"\n",
    "‚úî Lo·∫°i b·ªè: T·∫•t c·∫£ d·∫•u c√¢u trong m·ªói t·ª´.\n",
    "‚úî Lo·∫°i b·ªè: C√°c chu·ªói r·ªóng sau khi x·ª≠ l√Ω.\n",
    "‚úî Tr·∫£ v·ªÅ: Danh s√°ch t·ª´ s·∫°ch kh√¥ng ch·ª©a d·∫•u c√¢u.\"\"\"\n",
    "import nltk\n",
    "sample_text = \"The brown fox wasn't that quick and he couldn't win the race. Hey\"\n",
    "\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(text=sample_text)\n",
    "print(words)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "def remove_characters_after_tokenization(tokens):\n",
    "    import re # D√πng ƒë·ªÉ x·ª≠ l√Ω c√°c bi·ªÉu th·ª©c ch√≠nh quy (regular expressions)\n",
    "    import string # Ch·ª©a danh s√°ch c√°c d·∫•u c√¢u c√≥ s·∫µn trong Python (string.punctuation).\n",
    "\n",
    "    pattern = re.compile(\"[{}]\".format(re.escape(string.punctuation))) # string.punctuation ch·ª©a c√°c k√Ω t·ª± d·∫•u c√¢u nh∆∞ .,!?@#$%^&*()[]{};:'\"\n",
    "                                                                       # re.escape(string.punctuation) ƒë·∫£m b·∫£o r·∫±ng m·ªçi d·∫•u c√¢u ƒë∆∞·ª£c x·ª≠ l√Ω ƒë√∫ng (v√¨ m·ªôt s·ªë d·∫•u c√≥ th·ªÉ c√≥ √Ω nghƒ©a ƒë·∫∑c bi·ªát trong regex).\n",
    "                                                                       # re.compile(\"[{}]\".format(...)) t·∫°o m·ªôt m·∫´u regex ƒë·ªÉ t√¨m b·∫•t k·ª≥ d·∫•u c√¢u n√†o.\n",
    "    filtered_tokens = list(filter(None, [pattern.sub(\"\", token) for token in tokens])) \n",
    "    return filtered_tokens\n",
    "    \"\"\"\n",
    "    pattern.sub(\"\", token):\n",
    "\n",
    "    Thay th·∫ø t·∫•t c·∫£ d·∫•u c√¢u xu·∫•t hi·ªán trong token b·∫±ng chu·ªói r·ªóng (\"\").\n",
    "    Lo·∫°i b·ªè d·∫•u c√¢u kh·ªèi t·ª´.\n",
    "    List comprehension [pattern.sub(\"\", token) for token in tokens]:\n",
    "\n",
    "    √Åp d·ª•ng vi·ªác thay th·∫ø tr√™n to√†n b·ªô danh s√°ch tokens.\n",
    "    V√≠ d·ª•: [\"Hello,\", \"world!\"] ‚Üí [\"Hello\", \"world\"]\n",
    "    filter(None, [...]):\n",
    "\n",
    "    Lo·∫°i b·ªè c√°c chu·ªói r·ªóng kh·ªèi danh s√°ch k·∫øt qu·∫£.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "filter_list_1 = remove_characters_after_tokenization(words)\n",
    "print(filter_list_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the brown fox wasn't that quick and he couldn't win the race.\n",
      "THE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE.\n"
     ]
    }
   ],
   "source": [
    "# CODE 4.2\n",
    "sample_text = \"The brown FOX wasn't that quick and he couldn't win the RACE.\"\n",
    "# lowercase\n",
    "print(sample_text.lower())\n",
    "#uppercase\n",
    "print(sample_text.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "--------------------\n",
      "['The', 'brown', 'fox', 'was', 'nt', 'that', 'quick', 'and', 'he', 'could', 'nt', 'win', 'the', 'race', 'Hey']\n",
      "['The', 'brown', 'fox', 'nt', 'quick', 'could', 'nt', 'win', 'race', 'Hey']\n"
     ]
    }
   ],
   "source": [
    "# CODE 4.3\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "english_stopwords = nltk.corpus.stopwords.words(\"english\") # nltk.corpus.stopwords: Truy c·∫≠p t·∫≠p h·ª£p t·ª´ d·ª´ng c√≥ s·∫µn trong NLTK.\n",
    "                                                           # .words(\"english\"): L·∫•y danh s√°ch stopwords c·ªßa ti·∫øng Anh.\n",
    "print(len(english_stopwords))\n",
    "print(english_stopwords)\n",
    "print(\"-\"*20)\n",
    "\n",
    "def remove_stopwords(tokens, language = \"english\"):\n",
    "    stopword_list = nltk.corpus.stopwords.words(language)\n",
    "    filter_tokens = [token for token in tokens if token not in stopword_list] #Duy·ªát qua t·ª´ng token trong tokens\n",
    "                                                                              # Ch·ªâ gi·ªØ l·∫°i nh·ªØng t·ª´ kh√¥ng n·∫±m trong stopword_list\n",
    "    return filter_tokens\n",
    "\n",
    "print(filter_list_1)\n",
    "filter_list_2 = remove_stopwords(filter_list_1)\n",
    "print(filter_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Tr∆∞·ªõc khi ch·ªânh s·ª≠a: ['My', 'schooool', 'is', 'reallyyyyy', 'ammaaazinggg']\n",
      "Sau khi ch·ªânh s·ª≠a: ['My', 'school', 'is', 'really', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "# CODE 4.4\n",
    "\"\"\"\n",
    "Ch·ª©c nƒÉng: X√≥a k√Ω t·ª± l·∫∑p kh√¥ng c·∫ßn thi·∫øt trong t·ª´.\n",
    "D√πng WordNet ƒë·ªÉ ki·ªÉm tra nghƒ©a: Gi·ªØ nguy√™n t·ª´ n·∫øu c√≥ nghƒ©a.\n",
    "√Åp d·ª•ng ƒë·ªá quy: L·∫∑p l·∫°i s·ª≠a l·ªói cho ƒë·∫øn khi t·ª´ h·ª£p l·ªá.\n",
    "\"\"\"\n",
    "def remove_repeated_characters(tokens):\n",
    "    from nltk.corpus import wordnet\n",
    "    import re\n",
    "\n",
    "    \n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)') #(\\w*) ‚Üí B·∫•t k·ª≥ chu·ªói k√Ω t·ª± n√†o tr∆∞·ªõc k√Ω t·ª± b·ªã l·∫∑p.\n",
    "                                                     #(\\w)\\2 ‚Üí M·ªôt k√Ω t·ª± (\\w) xu·∫•t hi·ªán 2 l·∫ßn li√™n ti·∫øp (\\2).\n",
    "                                                     #(\\w*) ‚Üí Chu·ªói k√Ω t·ª± c√≤n l·∫°i sau k√Ω t·ª± l·∫∑p. \"aaammaazinggg\"\t\"amaazinggg\"\n",
    "    match_substitution = r'\\1\\2\\3' #Gi·ªØ l·∫°i ph·∫ßn ƒë·∫ßu (\\1), ch·ªâ gi·ªØ l·∫°i 1 k√Ω t·ª± l·∫∑p (\\2), v√† ph·∫ßn sau (\\3)  \"hellooo\"\t\"hello\"\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "import nltk\n",
    "sample_text = \"My schooool is reallyyyyy ammaaazinggg\"\n",
    "\n",
    "\n",
    "print(\"----\"*30)\n",
    "# Tokenize c√¢u th√†nh danh s√°ch t·ª´\n",
    "default_wt = nltk.word_tokenize\n",
    "tokens = default_wt(text=sample_text)\n",
    "print(\"Tr∆∞·ªõc khi ch·ªânh s·ª≠a:\", tokens)\n",
    "\n",
    "# √Åp d·ª•ng h√†m remove_repeated_characters\n",
    "sample_tokens = remove_repeated_characters(tokens)\n",
    "print(\"Sau khi ch·ªânh s·ª≠a:\", sample_tokens)\n",
    "\n",
    "# code n√†y d√†i qu√° c√¥ ui ƒë·ªÉ l√†m m·∫•y kia ƒë·ªçc l·∫°i n√†y sau hihi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 5\n",
    "\n",
    "Stemming v√† Lemmatization\n",
    "\n",
    "D∆∞·ªõi ƒë√¢y l√† b·∫£ng so s√°nh gi·ªØa **Stemming** v√† **Lemmatization**:  \n",
    "\n",
    "| **Ph∆∞∆°ng ph√°p**    | **C√°ch ho·∫°t ƒë·ªông**           | **K·∫øt qu·∫£**            | **∆Øu ƒëi·ªÉm**                  | **Nh∆∞·ª£c ƒëi·ªÉm**                     |\n",
    "|--------------------|-----------------------------|------------------------|------------------------------|-------------------------------------|\n",
    "| **Stemming**      | C·∫Øt g·ªçt h·∫≠u t·ªë b·∫±ng quy t·∫Øc  | `\"studies\"` ‚Üí `\"studi\"` | Nhanh, √≠t t·ªën t√†i nguy√™n     | Kh√¥ng t·∫°o ra t·ª´ c√≥ nghƒ©a           |\n",
    "| **Lemmatization** | Tra c·ª©u t·ª´ g·ªëc trong WordNet | `\"studies\"` ‚Üí `\"study\"` | Ch√≠nh x√°c, c√≥ nghƒ©a          | Ch·∫≠m h∆°n, c·∫ßn x√°c ƒë·ªãnh lo·∫°i t·ª´      |\n",
    "\n",
    "üöÄ **T√≥m l·∫°i**:\n",
    "- **Stemming**: Th√≠ch h·ª£p khi c·∫ßn t·ªëc ƒë·ªô, v√≠ d·ª• nh∆∞ c√¥ng c·ª• t√¨m ki·∫øm.\n",
    "- **Lemmatization**: Th√≠ch h·ª£p khi c·∫ßn ng·ªØ nghƒ©a ch√≠nh x√°c, v√≠ d·ª• trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n n√¢ng cao.\n",
    "\n",
    "üöÄ ·ª®ng d·ª•ng trong NLP:\n",
    "\n",
    "- Stemming: D√πng cho t√¨m ki·∫øm vƒÉn b·∫£n (Search Engine) ƒë·ªÉ gi·∫£m s·ªë l∆∞·ª£ng t·ª´.\n",
    "- Lemmatization: D√πng cho m√¥ h√¨nh AI/NLP n√¢ng cao, c·∫ßn ƒë·ªô ch√≠nh x√°c cao.\n",
    "\n",
    "üëâ N·∫øu mu·ªën nhanh, d√πng Stemming. N·∫øu mu·ªën ch√≠nh x√°c, d√πng Lemmatization. ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming:\n",
      "running -> run\n",
      "flies -> fli\n",
      "easily -> easili\n",
      "studies -> studi\n",
      "\n",
      "Lemmatization:\n",
      "running -> running\n",
      "flies -> fly\n",
      "easily -> easily\n",
      "studies -> study\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nLemmatization s·∫Ω t√¨m t·ª´ g·ªëc c√≥ nghƒ©a b·∫±ng c√°ch tham chi·∫øu WordNet.\\n\\n‚ö†Ô∏è L∆∞u √Ω:\\n\\nM·∫∑c ƒë·ªãnh Lemmatization kh√¥ng ho·∫°t ƒë·ªông t·ªët v·ªõi ƒë·ªông t·ª´ n·∫øu kh√¥ng cung c·∫•p lo·∫°i t·ª´.\\nN·∫øu mu·ªën lemmatize ch√≠nh x√°c, c·∫ßn ch·ªâ ƒë·ªãnh lo·∫°i t·ª´ (pos) cho WordNet.\\n\\nlemmatizer.lemmatize(\"running\", pos=\"v\")  # \"run\"\\nlemmatizer.lemmatize(\"studies\", pos=\"n\")  # \"study\"\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\"\"\"\n",
    "‚úÖ Th∆∞ vi·ªán nltk.stem ch·ª©a hai ph∆∞∆°ng ph√°p quan tr·ªçng trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP):\n",
    "\n",
    "PorterStemmer: D√πng ƒë·ªÉ stem (r√∫t g·ªçn t·ª´ v·ªÅ g·ªëc).\n",
    "WordNetLemmatizer: D√πng ƒë·ªÉ lemmatize (bi·∫øn t·ª´ v·ªÅ d·∫°ng c∆° b·∫£n/nguy√™n m·∫´u).\n",
    "\"\"\"\n",
    "nltk.download('wordnet') #WordNet l√† m·ªôt c∆° s·ªü d·ªØ li·ªáu t·ª´ v·ª±ng ti·∫øng Anh.ƒê∆∞·ª£c d√πng ƒë·ªÉ tra c·ª©u t·ª´ g·ªëc c·ªßa m·ªôt t·ª´ c√≥ nghƒ©a (lemma).\n",
    "\n",
    "stemmer = PorterStemmer() # stemmer: T·∫°o m·ªôt ƒë·ªëi t∆∞·ª£ng Stemming ƒë·ªÉ x·ª≠ l√Ω c√°c t·ª´.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # lemmatizer: T·∫°o m·ªôt ƒë·ªëi t∆∞·ª£ng Lemmatization ƒë·ªÉ t√¨m t·ª´ g·ªëc.\n",
    "\n",
    "\n",
    "words = [\"running\", \"flies\", \"easily\", \"studies\"]\n",
    "print(\"Stemming:\")\n",
    "for word in words:\n",
    "    print(word, \"->\", stemmer.stem(word))\n",
    "\n",
    "\"\"\"\n",
    " Stemming s·∫Ω c·∫Øt b·ªè h·∫≠u t·ªë c·ªßa t·ª´ ƒë·ªÉ ƒë∆∞a v·ªÅ g·ªëc t·ª´ g·∫ßn nh·∫•t.\n",
    "\n",
    "Kh√¥ng quan t√¢m ƒë·∫øn ng·ªØ nghƒ©a, ch·ªâ d·ª±a v√†o quy t·∫Øc c·∫Øt g·ªçt.\n",
    "\n",
    "Nh∆∞·ª£c ƒëi·ªÉm c·ªßa Stemming:\n",
    "\n",
    "Kh√¥ng t·∫°o ra t·ª´ c√≥ nghƒ©a.\n",
    "Ch·ªâ d·ª±a v√†o quy t·∫Øc c·∫Øt h·∫≠u t·ªë.\n",
    "\"\"\"  \n",
    "\n",
    "\n",
    "print(\"\\nLemmatization:\")\n",
    "for word in words:\n",
    "    print(word, \"->\", lemmatizer.lemmatize(word))\n",
    "\n",
    "\"\"\"\n",
    "Lemmatization s·∫Ω t√¨m t·ª´ g·ªëc c√≥ nghƒ©a b·∫±ng c√°ch tham chi·∫øu WordNet.\n",
    "\n",
    "‚ö†Ô∏è L∆∞u √Ω:\n",
    "\n",
    "M·∫∑c ƒë·ªãnh Lemmatization kh√¥ng ho·∫°t ƒë·ªông t·ªët v·ªõi ƒë·ªông t·ª´ n·∫øu kh√¥ng cung c·∫•p lo·∫°i t·ª´.\n",
    "N·∫øu mu·ªën lemmatize ch√≠nh x√°c, c·∫ßn ch·ªâ ƒë·ªãnh lo·∫°i t·ª´ (pos) cho WordNet.\n",
    "\n",
    "lemmatizer.lemmatize(\"running\", pos=\"v\")  # \"run\"\n",
    "lemmatizer.lemmatize(\"studies\", pos=\"n\")  # \"study\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE 6\n",
    "\n",
    "- T·ªïng k·∫øt\n",
    "\n",
    "Tokenization: T√°ch c√¢u th√†nh c√°c t·ª´/token.\n",
    "\n",
    "POS Tagging: G√°n nh√£n t·ª´ lo·∫°i cho t·ª´ng token.\n",
    "\n",
    "- ·ª®ng d·ª•ng:\n",
    "\n",
    "Ph√¢n t√≠ch c√∫ ph√°p c√¢u.\n",
    "\n",
    "Tr√≠ch xu·∫•t th√¥ng tin t·ª´ vƒÉn b·∫£n.\n",
    "\n",
    "H·ªó tr·ª£ cho b√†i to√°n chatbot, d·ªãch m√°y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging for sentence 1: [('NLP', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('interesting', 'JJ'), ('field', 'NN'), ('of', 'IN'), ('study', 'NN'), ('.', '.')]\n",
      "POS tagging for sentence 2: [('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# T·∫£i m√¥ h√¨nh c·∫ßn thi·∫øt\n",
    "#nltk.download('averaged_perceptron_tagger_eng') # Download the averaged_perceptron_tagger_eng # M√¥ h√¨nh g√°n nh√£n t·ª´ lo·∫°i (POS tagging).\n",
    "#nltk.download('punkt') # punkt: M√¥ h√¨nh d√πng ƒë·ªÉ t√°ch c√¢u v√† t·ª´.    \n",
    "\n",
    "# C√¢u ƒë·∫ßu v√†o\n",
    "sentence_1 = \"NLP is an interesting field of study.\"\n",
    "sentence_2 = \"The brown fox is quick and he is jumping over the lazy dog.\"\n",
    "\n",
    "# Tokenization\n",
    "words_1 = word_tokenize(sentence_1)\n",
    "words_2 = word_tokenize(sentence_2)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags_1 = nltk.pos_tag(words_1)\n",
    "pos_tags_2 = nltk.pos_tag(words_2)\n",
    "\"\"\"\n",
    "nltk.pos_tag(): X√°c ƒë·ªãnh t·ª´ lo·∫°i (danh t·ª´, ƒë·ªông t·ª´, t√≠nh t·ª´, v.v.).\n",
    "Tr·∫£ v·ªÅ danh s√°ch c√°c c·∫∑p (t·ª´, nh√£n t·ª´ lo·∫°i).\n",
    "\n",
    "\n",
    "Nh√£n\tNghƒ©a\n",
    "NN\t    Danh t·ª´ s·ªë √≠t (fox, dog, field)\n",
    "NNS\t    Danh t·ª´ s·ªë nhi·ªÅu (dogs, studies)\n",
    "VBZ\t    ƒê·ªông t·ª´ chia ·ªü ng√¥i th·ª© ba s·ªë √≠t (is, runs)\n",
    "VBG \tƒê·ªông t·ª´ d·∫°ng V-ing (jumping, running)\n",
    "JJ\t    T√≠nh t·ª´ (quick, interesting, lazy)\n",
    "DT\t    M·∫°o t·ª´ (the, an, a)\n",
    "IN\t    Gi·ªõi t·ª´ (of, over)\n",
    "CC\t    Li√™n t·ª´ (and, but)\n",
    "PRP\t    ƒê·∫°i t·ª´ nh√¢n x∆∞ng (he, she, it)\n",
    "\"\"\"\n",
    "\n",
    "# K·∫øt qu·∫£\n",
    "print(\"POS tagging for sentence 1:\", pos_tags_1)\n",
    "print(\"POS tagging for sentence 2:\", pos_tags_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"‚ÄúH√¥m nay l√† m·ªôt ng√†y ƒë·∫πp tr·ªùi ^^. Nh∆∞ng tui ph·∫£i ƒëi th·ª±c h√†nhhhhhhh m√¥n NLP @-@. T·ªëi nay t√¥i s·∫Ω ra ngo√†i ƒÉn kem ƒë·ªÉ x·∫£ stress sau m·ªôt ng√†y d√†i luy·ªán t·∫≠p v·ªõi c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω trong Nlp. D√π sao th√¨, m√¨nh c≈©ng ƒëang h·ªçc h·ªèi ƒë∆∞·ª£c nhi·ªÅu th·ª© m·ªõi m·∫ª. C≈©ng may l√† kh√¥ng ph·∫£i l·∫≠p tr√¨nh qu√° nhi·ªÅu!‚Äù\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01municodedata\u001b[39;00m \u001b[38;5;66;03m# 1\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcontractions\u001b[39;00m \u001b[38;5;66;03m# 2\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import unicodedata # 1\n",
    "import contractions # 2\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer # 4, 5\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords # 6\n",
    "#Th∆∞ vi·ªán Natural Language Toolkit (t·∫°m d·ªãch l√† B·ªô c√¥ng c·ª• Ng√¥n ng·ªØ T·ª± nhi√™n, vi·∫øt t·∫Øt NLTK) l√† m·ªôt n·ªÅn t·∫£ng d·∫´n ƒë·∫ßu ƒë·ªÉ x√¢y d·ª±ng c√°c ch∆∞∆°ng tr√¨nh Python l√†m vi·ªác v·ªõi d·ªØ li·ªáu ng√¥n ng·ªØ c·ªßa con ng∆∞·ªùi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'underthesea'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01municodedata\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munderthesea\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Danh s√°ch stopwords ti·∫øng Vi·ªát (c√≥ th·ªÉ m·ªü r·ªông)\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'underthesea'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import underthesea\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Danh s√°ch stopwords ti·∫øng Vi·ªát (c√≥ th·ªÉ m·ªü r·ªông)\n",
    "vietnamese_stopwords = set([\n",
    "    \"l√†\", \"m·ªôt\", \"nh∆∞ng\", \"ph·∫£i\", \"t√¥i\", \"tui\", \"nay\", \"ra\", \"c≈©ng\", \"ƒëang\", \n",
    "    \"v·ªõi\", \"c√°c\", \"th√¨\", \"l√†\", \"m√¨nh\", \"c√≥\", \"v√†\", \"trong\", \"ƒë∆∞·ª£c\", \"sau\"\n",
    "])\n",
    "\n",
    "def preprocess_text_vietnamese(text, remove_digits=True):\n",
    "    # 1. Chu·∫©n h√≥a d·∫•u ti·∫øng Vi·ªát v√† chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng\n",
    "    text = unicodedata.normalize('NFC', text).lower()\n",
    "    print(\"1Ô∏è Sau chu·∫©n h√≥a:\", text)\n",
    "\n",
    "    # 2. B·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát (gi·ªØ l·∫°i kho·∫£ng tr·∫Øng v√† ch·ªØ)\n",
    "    pattern = r'[^a-zA-Z0-9\\s√Ä-·ª∏√†-·ªπ]' if not remove_digits else r'[^\\w\\s√Ä-·ª∏√†-·ªπ]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    print(\"2Ô∏è Sau khi b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát:\", text)\n",
    "\n",
    "    # 3. T√°ch t·ª´ ti·∫øng Vi·ªát\n",
    "    words = underthesea.word_tokenize(text)\n",
    "    print(\"3Ô∏è Tokenized:\", words)\n",
    "\n",
    "    # 4. Lo·∫°i b·ªè stopwords ti·∫øng Vi·ªát\n",
    "    words = [word for word in words if word not in vietnamese_stopwords]\n",
    "    print(\"4Ô∏è Sau khi b·ªè stopwords:\", words)\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# D·ªØ li·ªáu ti·∫øng Vi·ªát\n",
    "data_vietnamese = \"‚ÄúH√¥m nay l√† m·ªôt ng√†y ƒë·∫πp tr·ªùi ^^. Nh∆∞ng tui ph·∫£i ƒëi th·ª±c h√†nhhhhhhh m√¥n NLP @-@. T·ªëi nay t√¥i s·∫Ω ra ngo√†i ƒÉn kem ƒë·ªÉ x·∫£ stress sau m·ªôt ng√†y d√†i luy·ªán t·∫≠p v·ªõi c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω trong Nlp. D√π sao th√¨, m√¨nh c≈©ng ƒëang h·ªçc h·ªèi ƒë∆∞·ª£c nhi·ªÅu th·ª© m·ªõi m·∫ª. C≈©ng may l√† kh√¥ng ph·∫£i l·∫≠p tr√¨nh qu√° nhi·ªÅu!‚Äù\"\n",
    "\n",
    "# Ch·∫°y h√†m\n",
    "processed_text = preprocess_text_vietnamese(data_vietnamese)\n",
    "print(\"\\nüî• VƒÉn b·∫£n sau x·ª≠ l√Ω:\", processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
