{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THỰC HÀNH BUỔI 2\n",
    "\n",
    "## TIỀN XỬ LÝ VĂN BẢN\n",
    "\n",
    "Họ tên và MSSV: Huỳnh Chí Phi Thuận\n",
    "\n",
    "Lớp: Ngày học: KHDL2211 ngày 27 tháng 2 năm 2025\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Thực hiện các bước tiền xử lý cơ bản\n",
    "\n",
    "## Sinh viên thực hiện các đoạn code trong file Lap2_NLP_Code và cho biết:\n",
    "- a) Kết quả.\n",
    "- b) Từng đoạn code đang thực hiện bước tiền xử lý nào. Giải thích quá trình xử lý."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n📌 nltk.download('punkt') giúp tách câu, tách từ.\\n📌 nltk.download('wordnet') hỗ trợ tra cứu từ điển.\\n📌 sent_tokenize() & word_tokenize() dùng để chia nhỏ văn bản.\\n📌 wordnet giúp xử lý từ vựng như tra nghĩa, từ đồng nghĩa, trái nghĩa\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')  # Cần cho sent_tokenize và word_tokenize\n",
    "nltk.download('wordnet')  # Cần cho wordnet\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\"\"\"\n",
    "📌 nltk.download('punkt') giúp tách câu, tách từ.\n",
    "📌 nltk.download('wordnet') hỗ trợ tra cứu từ điển.\n",
    "📌 sent_tokenize() & word_tokenize() dùng để chia nhỏ văn bản.\n",
    "📌 wordnet giúp xử lý từ vựng như tra nghĩa, từ đồng nghĩa, trái nghĩa\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE 1\n",
    "\n",
    "Chức năng\tMô tả\n",
    "\n",
    "sent_tokenize(my_text)\tChia văn bản thành câu\n",
    "\n",
    "word_tokenize(my_text)\tChia văn bản thành từ\n",
    "\n",
    "list(my_text)\tChuyển văn bản thành danh sách ký tự\n",
    "\n",
    "wordnet.synsets(\"NLP\")\tTra cứu nghĩa của từ \"NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there.', 'Welcome to new semester.', 'How are you?', 'Today is a good.']\n",
      "1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-\n",
      "['Hello', 'there', '.', 'Welcome', 'to', 'new', 'semester', '.', 'How', 'are', 'you', '?', 'Today', 'is', 'a', 'good', '.']\n",
      "--------------------------------------------------\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 't', 'h', 'e', 'r', 'e', '.', ' ', 'W', 'e', 'l', 'c', 'o', 'm', 'e', ' ', 't', 'o', ' ', 'n', 'e', 'w', ' ', 's', 'e', 'm', 'e', 's', 't', 'e', 'r', '.', ' ', 'H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?', ' ', 'T', 'o', 'd', 'a', 'y', ' ', 'i', 's', ' ', 'a', ' ', 'g', 'o', 'o', 'd', '.']\n",
      "--------------------------------------------------\n",
      "the branch of information science that deals with natural language information\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  # Sửa lỗi ở đây\n",
    "    my_text = \"Hello there. Welcome to new semester. How are you? Today is a good.\"\n",
    "\n",
    "    print(sent_tokenize(my_text)) # này được tách ra theo từng dấu câu sau nó như !  . ? á. theo từng câu riêng biệt\n",
    "    print(\"1-\"*50) # màu mè nữa\n",
    "\n",
    "    print(word_tokenize(my_text)) # đoạn này tách từng câu ra luôn dâu , ? cũng tách nữa\n",
    "    print(\"-\"*50) # màu mè nữa\n",
    "\n",
    "    print(list(my_text)) # danh sách mà nó kì kì ta. tách ra từng chữ cái luôn\n",
    "    print(\"-\"*50) # màu mè nữa\n",
    "\n",
    "    syn = wordnet.synsets(\"NLP\")\n",
    "    if syn:\n",
    "        print(syn[0].definition())\n",
    "    else:\n",
    "        print(\"no definition found for NLP.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 66\n",
      "Total sentences in sample_sentences: 1\n",
      "sentence 1: We will discuss briefly about the basic syntax, structure and desi\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sample_text = \"We will discuss briefly about the basic syntax, structure and desi\"\n",
    "\n",
    "print(\"Total characters: \" + str(len(sample_text))) # số kí tự trong chuỗi bao gồm dấu cách\n",
    "\n",
    "# import sentence tokenization\n",
    "default_st = nltk.sent_tokenize\n",
    "\n",
    "# apply on the sample_text\n",
    "sample_sentences = default_st(text=sample_text) # tách ra từng câu riêng biệt\n",
    "# print(sent_tokenize(sample_text))\n",
    "\n",
    "print(\"Total sentences in sample_sentences: \" + str(len(sample_sentences)))\n",
    "for i in range (len(sample_sentences)):\n",
    "    print(\"sentence \" + str(i+1) + \": \" + sample_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:  144395\n",
      "--------------------------------------------------\n",
      "First 100 characters in the corpus\n",
      "\n",
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n",
      "--------------------------------------------------\n",
      "Total sentences in alice:  1625\n",
      "--------------------------------------------------\n",
      "First 2 sentences in alice: \n",
      "sentence 1: [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I.\n",
      "sentence 2: Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Download the 'gutenberg' corpus #Tải xuống và đọc nội dung của văn bản Alice’s Adventures in Wonderland từ tập dữ liệu gutenberg trong nltk\n",
    "#nltk.download('gutenberg')\n",
    "\n",
    "alice = gutenberg.raw(fileids=\"carroll-alice.txt\")\n",
    "print(\"Total characters: \", str(len(alice)))\n",
    "\n",
    "print(\"-\"*50) # màu mè nữa\n",
    "\n",
    "\n",
    "print(\"First 100 characters in the corpus\\n\")\n",
    "print(alice[0:100])\n",
    "\n",
    "print(\"-\"*50) # màu mè nữa\n",
    "\n",
    "## import sentence tokenization\n",
    "default_st = nltk.sent_tokenize # tách ra từng câu riêng biệt\n",
    "\n",
    "# apply on the sample_text\n",
    "alice_sentences = default_st(text=alice) # tách ra từng câu riêng biệt\n",
    "\n",
    "print(\"Total sentences in alice: \", str(len(alice_sentences)))\n",
    "print(\"-\"*50)\n",
    "print(\"First 2 sentences in alice: \")\n",
    "print(\"sentence 1: \" + alice_sentences[0])\n",
    "print(\"sentence 2: \" + alice_sentences[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "['The', 'brown', 'fox', 'wasn', '’', 't', 'that', 'quick', 'and', 'he', 'couldn', '’', 't', 'win', 'the', 'race', '.', 'Hey', 'that', '’', 's', 'a', 'great', 'deal', '!', 'I', 'just', 'bought', 'a', 'phone', 'for', '$', '199', '.', 'You', '’', 'll', 'learn', 'a', 'lot', 'in', 'the', 'BOOK', '.', 'Python', 'is', 'an', 'amazing', 'language', '!']\n",
      "--------------------------------------------------\n",
      "Cách để giữ nguyên contractions (không tách wasn't thành was  n't)\n",
      "36\n",
      "['The', 'brown', 'fox', 'wasn’t', 'that', 'quick', 'and', 'he', 'couldn’t', 'win', 'the', 'race.', 'Hey', 'that’s', 'a', 'great', 'deal!', 'I', 'just', 'bought', 'a', 'phone', 'for', '$199.', 'You’ll', 'learn', 'a', 'lot', 'in', 'the', 'BOOK.', 'Python', 'is', 'an', 'amazing', 'language!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sample_text = \"The brown fox wasn’t that quick and he couldn’t win the race. Hey that’s a great deal! I just bought a phone for $199. You’ll learn a lot in the BOOK. Python is an amazing language!\"\n",
    "\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(text=sample_text)\n",
    "\n",
    "print(len(words)) # đếm số từ \n",
    "print(words) \n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"Cách để giữ nguyên contractions (không tách wasn't thành was  n't)\")\n",
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sample_text)\n",
    "print(len(words)) # đếm số từ \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race', '.', 'Hey']\n",
      "--------------------\n",
      "['The', 'brown', 'fox', 'was', 'nt', 'that', 'quick', 'and', 'he', 'could', 'nt', 'win', 'the', 'race', 'Hey']\n"
     ]
    }
   ],
   "source": [
    "# code 4.1\n",
    "\"\"\"\n",
    "✔ Loại bỏ: Tất cả dấu câu trong mỗi từ.\n",
    "✔ Loại bỏ: Các chuỗi rỗng sau khi xử lý.\n",
    "✔ Trả về: Danh sách từ sạch không chứa dấu câu.\"\"\"\n",
    "import nltk\n",
    "sample_text = \"The brown fox wasn't that quick and he couldn't win the race. Hey\"\n",
    "\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(text=sample_text)\n",
    "print(words)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "def remove_characters_after_tokenization(tokens):\n",
    "    import re # Dùng để xử lý các biểu thức chính quy (regular expressions)\n",
    "    import string # Chứa danh sách các dấu câu có sẵn trong Python (string.punctuation).\n",
    "\n",
    "    pattern = re.compile(\"[{}]\".format(re.escape(string.punctuation))) # string.punctuation chứa các ký tự dấu câu như .,!?@#$%^&*()[]{};:'\"\n",
    "                                                                       # re.escape(string.punctuation) đảm bảo rằng mọi dấu câu được xử lý đúng (vì một số dấu có thể có ý nghĩa đặc biệt trong regex).\n",
    "                                                                       # re.compile(\"[{}]\".format(...)) tạo một mẫu regex để tìm bất kỳ dấu câu nào.\n",
    "    filtered_tokens = list(filter(None, [pattern.sub(\"\", token) for token in tokens])) \n",
    "    return filtered_tokens\n",
    "    \"\"\"\n",
    "    pattern.sub(\"\", token):\n",
    "\n",
    "    Thay thế tất cả dấu câu xuất hiện trong token bằng chuỗi rỗng (\"\").\n",
    "    Loại bỏ dấu câu khỏi từ.\n",
    "    List comprehension [pattern.sub(\"\", token) for token in tokens]:\n",
    "\n",
    "    Áp dụng việc thay thế trên toàn bộ danh sách tokens.\n",
    "    Ví dụ: [\"Hello,\", \"world!\"] → [\"Hello\", \"world\"]\n",
    "    filter(None, [...]):\n",
    "\n",
    "    Loại bỏ các chuỗi rỗng khỏi danh sách kết quả.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "filter_list_1 = remove_characters_after_tokenization(words)\n",
    "print(filter_list_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the brown fox wasn't that quick and he couldn't win the race.\n",
      "THE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE.\n"
     ]
    }
   ],
   "source": [
    "# CODE 4.2\n",
    "sample_text = \"The brown FOX wasn't that quick and he couldn't win the RACE.\"\n",
    "# lowercase\n",
    "print(sample_text.lower())\n",
    "#uppercase\n",
    "print(sample_text.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "--------------------\n",
      "['The', 'brown', 'fox', 'was', 'nt', 'that', 'quick', 'and', 'he', 'could', 'nt', 'win', 'the', 'race', 'Hey']\n",
      "['The', 'brown', 'fox', 'nt', 'quick', 'could', 'nt', 'win', 'race', 'Hey']\n"
     ]
    }
   ],
   "source": [
    "# CODE 4.3\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "english_stopwords = nltk.corpus.stopwords.words(\"english\") # nltk.corpus.stopwords: Truy cập tập hợp từ dừng có sẵn trong NLTK.\n",
    "                                                           # .words(\"english\"): Lấy danh sách stopwords của tiếng Anh.\n",
    "print(len(english_stopwords))\n",
    "print(english_stopwords)\n",
    "print(\"-\"*20)\n",
    "\n",
    "def remove_stopwords(tokens, language = \"english\"):\n",
    "    stopword_list = nltk.corpus.stopwords.words(language)\n",
    "    filter_tokens = [token for token in tokens if token not in stopword_list] #Duyệt qua từng token trong tokens\n",
    "                                                                              # Chỉ giữ lại những từ không nằm trong stopword_list\n",
    "    return filter_tokens\n",
    "\n",
    "print(filter_list_1)\n",
    "filter_list_2 = remove_stopwords(filter_list_1)\n",
    "print(filter_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Trước khi chỉnh sửa: ['My', 'schooool', 'is', 'reallyyyyy', 'ammaaazinggg']\n",
      "Sau khi chỉnh sửa: ['My', 'school', 'is', 'really', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "# CODE 4.4\n",
    "\"\"\"\n",
    "Chức năng: Xóa ký tự lặp không cần thiết trong từ.\n",
    "Dùng WordNet để kiểm tra nghĩa: Giữ nguyên từ nếu có nghĩa.\n",
    "Áp dụng đệ quy: Lặp lại sửa lỗi cho đến khi từ hợp lệ.\n",
    "\"\"\"\n",
    "def remove_repeated_characters(tokens):\n",
    "    from nltk.corpus import wordnet\n",
    "    import re\n",
    "\n",
    "    \n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)') #(\\w*) → Bất kỳ chuỗi ký tự nào trước ký tự bị lặp.\n",
    "                                                     #(\\w)\\2 → Một ký tự (\\w) xuất hiện 2 lần liên tiếp (\\2).\n",
    "                                                     #(\\w*) → Chuỗi ký tự còn lại sau ký tự lặp. \"aaammaazinggg\"\t\"amaazinggg\"\n",
    "    match_substitution = r'\\1\\2\\3' #Giữ lại phần đầu (\\1), chỉ giữ lại 1 ký tự lặp (\\2), và phần sau (\\3)  \"hellooo\"\t\"hello\"\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "import nltk\n",
    "sample_text = \"My schooool is reallyyyyy ammaaazinggg\"\n",
    "\n",
    "\n",
    "print(\"----\"*30)\n",
    "# Tokenize câu thành danh sách từ\n",
    "default_wt = nltk.word_tokenize\n",
    "tokens = default_wt(text=sample_text)\n",
    "print(\"Trước khi chỉnh sửa:\", tokens)\n",
    "\n",
    "# Áp dụng hàm remove_repeated_characters\n",
    "sample_tokens = remove_repeated_characters(tokens)\n",
    "print(\"Sau khi chỉnh sửa:\", sample_tokens)\n",
    "\n",
    "# code này dài quá cô ui để làm mấy kia đọc lại này sau hihi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 5\n",
    "\n",
    "Stemming và Lemmatization\n",
    "\n",
    "Dưới đây là bảng so sánh giữa **Stemming** và **Lemmatization**:  \n",
    "\n",
    "| **Phương pháp**    | **Cách hoạt động**           | **Kết quả**            | **Ưu điểm**                  | **Nhược điểm**                     |\n",
    "|--------------------|-----------------------------|------------------------|------------------------------|-------------------------------------|\n",
    "| **Stemming**      | Cắt gọt hậu tố bằng quy tắc  | `\"studies\"` → `\"studi\"` | Nhanh, ít tốn tài nguyên     | Không tạo ra từ có nghĩa           |\n",
    "| **Lemmatization** | Tra cứu từ gốc trong WordNet | `\"studies\"` → `\"study\"` | Chính xác, có nghĩa          | Chậm hơn, cần xác định loại từ      |\n",
    "\n",
    "🚀 **Tóm lại**:\n",
    "- **Stemming**: Thích hợp khi cần tốc độ, ví dụ như công cụ tìm kiếm.\n",
    "- **Lemmatization**: Thích hợp khi cần ngữ nghĩa chính xác, ví dụ trong xử lý ngôn ngữ tự nhiên nâng cao.\n",
    "\n",
    "🚀 Ứng dụng trong NLP:\n",
    "\n",
    "- Stemming: Dùng cho tìm kiếm văn bản (Search Engine) để giảm số lượng từ.\n",
    "- Lemmatization: Dùng cho mô hình AI/NLP nâng cao, cần độ chính xác cao.\n",
    "\n",
    "👉 Nếu muốn nhanh, dùng Stemming. Nếu muốn chính xác, dùng Lemmatization. ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming:\n",
      "running -> run\n",
      "flies -> fli\n",
      "easily -> easili\n",
      "studies -> studi\n",
      "\n",
      "Lemmatization:\n",
      "running -> running\n",
      "flies -> fly\n",
      "easily -> easily\n",
      "studies -> study\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nLemmatization sẽ tìm từ gốc có nghĩa bằng cách tham chiếu WordNet.\\n\\n⚠️ Lưu ý:\\n\\nMặc định Lemmatization không hoạt động tốt với động từ nếu không cung cấp loại từ.\\nNếu muốn lemmatize chính xác, cần chỉ định loại từ (pos) cho WordNet.\\n\\nlemmatizer.lemmatize(\"running\", pos=\"v\")  # \"run\"\\nlemmatizer.lemmatize(\"studies\", pos=\"n\")  # \"study\"\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\"\"\"\n",
    "✅ Thư viện nltk.stem chứa hai phương pháp quan trọng trong xử lý ngôn ngữ tự nhiên (NLP):\n",
    "\n",
    "PorterStemmer: Dùng để stem (rút gọn từ về gốc).\n",
    "WordNetLemmatizer: Dùng để lemmatize (biến từ về dạng cơ bản/nguyên mẫu).\n",
    "\"\"\"\n",
    "nltk.download('wordnet') #WordNet là một cơ sở dữ liệu từ vựng tiếng Anh.Được dùng để tra cứu từ gốc của một từ có nghĩa (lemma).\n",
    "\n",
    "stemmer = PorterStemmer() # stemmer: Tạo một đối tượng Stemming để xử lý các từ.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # lemmatizer: Tạo một đối tượng Lemmatization để tìm từ gốc.\n",
    "\n",
    "\n",
    "words = [\"running\", \"flies\", \"easily\", \"studies\"]\n",
    "print(\"Stemming:\")\n",
    "for word in words:\n",
    "    print(word, \"->\", stemmer.stem(word))\n",
    "\n",
    "\"\"\"\n",
    " Stemming sẽ cắt bỏ hậu tố của từ để đưa về gốc từ gần nhất.\n",
    "\n",
    "Không quan tâm đến ngữ nghĩa, chỉ dựa vào quy tắc cắt gọt.\n",
    "\n",
    "Nhược điểm của Stemming:\n",
    "\n",
    "Không tạo ra từ có nghĩa.\n",
    "Chỉ dựa vào quy tắc cắt hậu tố.\n",
    "\"\"\"  \n",
    "\n",
    "\n",
    "print(\"\\nLemmatization:\")\n",
    "for word in words:\n",
    "    print(word, \"->\", lemmatizer.lemmatize(word))\n",
    "\n",
    "\"\"\"\n",
    "Lemmatization sẽ tìm từ gốc có nghĩa bằng cách tham chiếu WordNet.\n",
    "\n",
    "⚠️ Lưu ý:\n",
    "\n",
    "Mặc định Lemmatization không hoạt động tốt với động từ nếu không cung cấp loại từ.\n",
    "Nếu muốn lemmatize chính xác, cần chỉ định loại từ (pos) cho WordNet.\n",
    "\n",
    "lemmatizer.lemmatize(\"running\", pos=\"v\")  # \"run\"\n",
    "lemmatizer.lemmatize(\"studies\", pos=\"n\")  # \"study\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE 6\n",
    "\n",
    "- Tổng kết\n",
    "\n",
    "Tokenization: Tách câu thành các từ/token.\n",
    "\n",
    "POS Tagging: Gán nhãn từ loại cho từng token.\n",
    "\n",
    "- Ứng dụng:\n",
    "\n",
    "Phân tích cú pháp câu.\n",
    "\n",
    "Trích xuất thông tin từ văn bản.\n",
    "\n",
    "Hỗ trợ cho bài toán chatbot, dịch máy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging for sentence 1: [('NLP', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('interesting', 'JJ'), ('field', 'NN'), ('of', 'IN'), ('study', 'NN'), ('.', '.')]\n",
      "POS tagging for sentence 2: [('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tải mô hình cần thiết\n",
    "#nltk.download('averaged_perceptron_tagger_eng') # Download the averaged_perceptron_tagger_eng # Mô hình gán nhãn từ loại (POS tagging).\n",
    "#nltk.download('punkt') # punkt: Mô hình dùng để tách câu và từ.    \n",
    "\n",
    "# Câu đầu vào\n",
    "sentence_1 = \"NLP is an interesting field of study.\"\n",
    "sentence_2 = \"The brown fox is quick and he is jumping over the lazy dog.\"\n",
    "\n",
    "# Tokenization\n",
    "words_1 = word_tokenize(sentence_1)\n",
    "words_2 = word_tokenize(sentence_2)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags_1 = nltk.pos_tag(words_1)\n",
    "pos_tags_2 = nltk.pos_tag(words_2)\n",
    "\"\"\"\n",
    "nltk.pos_tag(): Xác định từ loại (danh từ, động từ, tính từ, v.v.).\n",
    "Trả về danh sách các cặp (từ, nhãn từ loại).\n",
    "\n",
    "\n",
    "Nhãn\tNghĩa\n",
    "NN\t    Danh từ số ít (fox, dog, field)\n",
    "NNS\t    Danh từ số nhiều (dogs, studies)\n",
    "VBZ\t    Động từ chia ở ngôi thứ ba số ít (is, runs)\n",
    "VBG \tĐộng từ dạng V-ing (jumping, running)\n",
    "JJ\t    Tính từ (quick, interesting, lazy)\n",
    "DT\t    Mạo từ (the, an, a)\n",
    "IN\t    Giới từ (of, over)\n",
    "CC\t    Liên từ (and, but)\n",
    "PRP\t    Đại từ nhân xưng (he, she, it)\n",
    "\"\"\"\n",
    "\n",
    "# Kết quả\n",
    "print(\"POS tagging for sentence 1:\", pos_tags_1)\n",
    "print(\"POS tagging for sentence 2:\", pos_tags_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiền xử lý văn bản tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"“Hôm nay là một ngày đẹp trời ^^. Nhưng tui phải đi thực hànhhhhhhh môn NLP @-@. Tối nay tôi sẽ ra ngoài ăn kem để xả stress sau một ngày dài luyện tập với các bước tiền xử lý trong Nlp. Dù sao thì, mình cũng đang học hỏi được nhiều thứ mới mẻ. Cũng may là không phải lập trình quá nhiều!”\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01municodedata\u001b[39;00m \u001b[38;5;66;03m# 1\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcontractions\u001b[39;00m \u001b[38;5;66;03m# 2\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import unicodedata # 1\n",
    "import contractions # 2\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer # 4, 5\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords # 6\n",
    "#Thư viện Natural Language Toolkit (tạm dịch là Bộ công cụ Ngôn ngữ Tự nhiên, viết tắt NLTK) là một nền tảng dẫn đầu để xây dựng các chương trình Python làm việc với dữ liệu ngôn ngữ của con người"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'underthesea'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01municodedata\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munderthesea\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Danh sách stopwords tiếng Việt (có thể mở rộng)\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'underthesea'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import underthesea\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Danh sách stopwords tiếng Việt (có thể mở rộng)\n",
    "vietnamese_stopwords = set([\n",
    "    \"là\", \"một\", \"nhưng\", \"phải\", \"tôi\", \"tui\", \"nay\", \"ra\", \"cũng\", \"đang\", \n",
    "    \"với\", \"các\", \"thì\", \"là\", \"mình\", \"có\", \"và\", \"trong\", \"được\", \"sau\"\n",
    "])\n",
    "\n",
    "def preprocess_text_vietnamese(text, remove_digits=True):\n",
    "    # 1. Chuẩn hóa dấu tiếng Việt và chuyển về chữ thường\n",
    "    text = unicodedata.normalize('NFC', text).lower()\n",
    "    print(\"1️ Sau chuẩn hóa:\", text)\n",
    "\n",
    "    # 2. Bỏ ký tự đặc biệt (giữ lại khoảng trắng và chữ)\n",
    "    pattern = r'[^a-zA-Z0-9\\sÀ-Ỹà-ỹ]' if not remove_digits else r'[^\\w\\sÀ-Ỹà-ỹ]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    print(\"2️ Sau khi bỏ ký tự đặc biệt:\", text)\n",
    "\n",
    "    # 3. Tách từ tiếng Việt\n",
    "    words = underthesea.word_tokenize(text)\n",
    "    print(\"3️ Tokenized:\", words)\n",
    "\n",
    "    # 4. Loại bỏ stopwords tiếng Việt\n",
    "    words = [word for word in words if word not in vietnamese_stopwords]\n",
    "    print(\"4️ Sau khi bỏ stopwords:\", words)\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Dữ liệu tiếng Việt\n",
    "data_vietnamese = \"“Hôm nay là một ngày đẹp trời ^^. Nhưng tui phải đi thực hànhhhhhhh môn NLP @-@. Tối nay tôi sẽ ra ngoài ăn kem để xả stress sau một ngày dài luyện tập với các bước tiền xử lý trong Nlp. Dù sao thì, mình cũng đang học hỏi được nhiều thứ mới mẻ. Cũng may là không phải lập trình quá nhiều!”\"\n",
    "\n",
    "# Chạy hàm\n",
    "processed_text = preprocess_text_vietnamese(data_vietnamese)\n",
    "print(\"\\n🔥 Văn bản sau xử lý:\", processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
