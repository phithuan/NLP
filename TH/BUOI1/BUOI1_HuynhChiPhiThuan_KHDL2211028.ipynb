{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B√ÄI T·∫¨P 1\n",
    "Sinh vi√™n t√¨m hi·ªÉu c√°c th∆∞ vi·ªán n√™u tr√™n (ƒë·∫∑c ƒëi·ªÉm, ch·ª©c nƒÉng, ƒëi·ªÉm n·ªïi b·∫≠t, c√°ch th·ª©c v√† l·ªánh c√†i ƒë·∫∑t) v√† cho bi·∫øt th∆∞ vi·ªán n√†o h·ªó tr·ª£ cho NLP.\n",
    "\n",
    "Sinh vi√™n nh·∫≠p c√¢u tr·∫£ l·ªùi t·∫°i ƒë√¢y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danh s√°ch c√°c th∆∞ vi·ªán theo nh√≥m\n",
    "1. Th∆∞ vi·ªán t√≠nh to√°n v√† Machine Learning\n",
    "NumPy\n",
    "Keras\n",
    "Pandas\n",
    "PyTorch\n",
    "TensorFlow\n",
    "\n",
    "2. Th∆∞ vi·ªán v·∫Ω ƒë·ªì th·ªã v√† x·ª≠ l√Ω ·∫£nh\n",
    "Matplotlib\n",
    "OpenCV\n",
    "Seaborn\n",
    "Scipy\n",
    "Theano\n",
    "SimpleITK\n",
    "Pillow\n",
    "\n",
    "3. Th∆∞ vi·ªán Web Scraping\n",
    "Selenium\n",
    "Scrapy\n",
    "Requests\n",
    "Beautiful Soup\n",
    "\n",
    "4. Th∆∞ vi·ªán ki·ªÉm th·ª≠\n",
    "Pytest\n",
    "Mahotas\n",
    "\n",
    "5. Th∆∞ vi·ªán h·ªó tr·ª£ NLP\n",
    "NLTK\n",
    "Scikit-Learn\n",
    "Gensim\n",
    "TextBlob\n",
    "SpaCy\n",
    "Pattern\n",
    "Transformers\n",
    "\n",
    "\n",
    "D∆∞·ªõi ƒë√¢y l√† b·∫£ng t·ªïng h·ª£p th√¥ng tin v·ªÅ ƒë·∫∑c ƒëi·ªÉm, ch·ª©c nƒÉng, ƒëi·ªÉm n·ªïi b·∫≠t v√† c√°ch th·ª©c c√†i ƒë·∫∑t c·ªßa c√°c th∆∞ vi·ªán ƒë√£ li·ªát k√™.  \n",
    "\n",
    "---\n",
    "\n",
    "### **1. Th∆∞ vi·ªán t√≠nh to√°n v√† Machine Learning**  \n",
    "| **Th∆∞ vi·ªán**  | **ƒê·∫∑c ƒëi·ªÉm**  | **Ch·ª©c nƒÉng**  | **ƒêi·ªÉm n·ªïi b·∫≠t**  | **C√°ch c√†i ƒë·∫∑t**  |\n",
    "|--------------|-------------|---------------|----------------|----------------|\n",
    "| **NumPy**  | X·ª≠ l√Ω s·ªë h·ªçc v√† ma tr·∫≠n | Cung c·∫•p m·∫£ng ƒëa chi·ªÅu, ph√©p to√°n tuy·∫øn t√≠nh | Hi·ªáu su·∫•t cao, s·ª≠ d·ª•ng trong ML & AI | `pip install numpy` |\n",
    "| **Keras**  | Framework Deep Learning | X√¢y d·ª±ng v√† hu·∫•n luy·ªán m√¥ h√¨nh h·ªçc s√¢u | Giao di·ªán d·ªÖ d√πng, t√≠ch h·ª£p v·ªõi TensorFlow | `pip install keras` |\n",
    "| **Pandas**  | X·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng b·∫£ng | Qu·∫£n l√Ω v√† ph√¢n t√≠ch d·ªØ li·ªáu | H·ªó tr·ª£ DataFrame m·∫°nh m·∫Ω | `pip install pandas` |\n",
    "| **PyTorch**  | Th∆∞ vi·ªán Deep Learning | X√¢y d·ª±ng v√† hu·∫•n luy·ªán m√¥ h√¨nh AI | Linh ho·∫°t, t·ªëi ∆∞u GPU | `pip install torch` |\n",
    "| **TensorFlow**  | Deep Learning | X√¢y d·ª±ng v√† tri·ªÉn khai m√¥ h√¨nh h·ªçc s√¢u | Google ph√°t tri·ªÉn, h·ªó tr·ª£ TPU | `pip install tensorflow` |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Th∆∞ vi·ªán v·∫Ω ƒë·ªì th·ªã v√† x·ª≠ l√Ω ·∫£nh**  \n",
    "| **Th∆∞ vi·ªán**  | **ƒê·∫∑c ƒëi·ªÉm**  | **Ch·ª©c nƒÉng**  | **ƒêi·ªÉm n·ªïi b·∫≠t**  | **C√°ch c√†i ƒë·∫∑t**  |\n",
    "|--------------|-------------|---------------|----------------|----------------|\n",
    "| **Matplotlib**  | V·∫Ω ƒë·ªì th·ªã 2D | Bi·ªÉu ƒë·ªì, ƒë·ªì th·ªã tr·ª±c quan | Tu·ª≥ ch·ªânh cao, d·ªÖ s·ª≠ d·ª•ng | `pip install matplotlib` |\n",
    "| **OpenCV**  | X·ª≠ l√Ω ·∫£nh/video | Nh·∫≠n di·ªán, ph√¢n t√≠ch ·∫£nh | M·∫°nh m·∫Ω, h·ªó tr·ª£ nhi·ªÅu ƒë·ªãnh d·∫°ng | `pip install opencv-python` |\n",
    "| **Seaborn**  | V·∫Ω bi·ªÉu ƒë·ªì th·ªëng k√™ | Ph√¢n t√≠ch d·ªØ li·ªáu tr·ª±c quan | Giao di·ªán ƒë·∫πp, d·ªÖ t√πy ch·ªânh | `pip install seaborn` |\n",
    "| **Scipy**  | T√≠nh to√°n khoa h·ªçc | Ph√©p to√°n ma tr·∫≠n, x·ª≠ l√Ω t√≠n hi·ªáu | M·ªü r·ªông t·ª´ NumPy | `pip install scipy` |\n",
    "| **Theano**  | T√≠nh to√°n tensor | X√¢y d·ª±ng m√¥ h√¨nh Deep Learning | Hi·ªáu su·∫•t cao, t·ªëi ∆∞u GPU | `pip install theano` |\n",
    "| **SimpleITK**  | X·ª≠ l√Ω ·∫£nh y t·∫ø | L√†m vi·ªác v·ªõi ·∫£nh DICOM | H·ªó tr·ª£ d·ªØ li·ªáu y t·∫ø | `pip install SimpleITK` |\n",
    "| **Pillow**  | X·ª≠ l√Ω ·∫£nh | Ch·ªânh s·ª≠a, chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng ·∫£nh | H·ªó tr·ª£ nhi·ªÅu lo·∫°i file ·∫£nh | `pip install pillow` |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Th∆∞ vi·ªán Web Scraping**  \n",
    "| **Th∆∞ vi·ªán**  | **ƒê·∫∑c ƒëi·ªÉm**  | **Ch·ª©c nƒÉng**  | **ƒêi·ªÉm n·ªïi b·∫≠t**  | **C√°ch c√†i ƒë·∫∑t**  |\n",
    "|--------------|-------------|---------------|----------------|----------------|\n",
    "| **Selenium**  | ƒêi·ªÅu khi·ªÉn tr√¨nh duy·ªát | T·ª± ƒë·ªông ho√° thao t√°c web | H·ªó tr·ª£ nhi·ªÅu tr√¨nh duy·ªát | `pip install selenium` |\n",
    "| **Scrapy**  | Web crawling | Thu th·∫≠p d·ªØ li·ªáu web t·ª± ƒë·ªông | T·ªëc ƒë·ªô nhanh, m·∫°nh m·∫Ω | `pip install scrapy` |\n",
    "| **Requests**  | HTTP request | G·ª≠i y√™u c·∫ßu HTTP/HTTPS | ƒê∆°n gi·∫£n, ph·ªï bi·∫øn | `pip install requests` |\n",
    "| **BeautifulSoup**  | Ph√¢n t√≠ch HTML | Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ web | Nh·∫π, d·ªÖ s·ª≠ d·ª•ng | `pip install beautifulsoup4` |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Th∆∞ vi·ªán ki·ªÉm th·ª≠**  \n",
    "| **Th∆∞ vi·ªán**  | **ƒê·∫∑c ƒëi·ªÉm**  | **Ch·ª©c nƒÉng**  | **ƒêi·ªÉm n·ªïi b·∫≠t**  | **C√°ch c√†i ƒë·∫∑t**  |\n",
    "|--------------|-------------|---------------|----------------|----------------|\n",
    "| **Pytest**  | Ki·ªÉm th·ª≠ t·ª± ƒë·ªông | Ch·∫°y unit test | ƒê∆°n gi·∫£n, m·ªü r·ªông d·ªÖ d√†ng | `pip install pytest` |\n",
    "| **Mahotas**  | X·ª≠ l√Ω ·∫£nh | Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng ·∫£nh | H·ªó tr·ª£ ph√¢n t√≠ch ·∫£nh nhanh | `pip install mahotas` |\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Th∆∞ vi·ªán h·ªó tr·ª£ NLP**  \n",
    "| **Th∆∞ vi·ªán**  | **ƒê·∫∑c ƒëi·ªÉm**  | **Ch·ª©c nƒÉng**  | **ƒêi·ªÉm n·ªïi b·∫≠t**  | **C√°ch c√†i ƒë·∫∑t**  |\n",
    "|--------------|-------------|---------------|----------------|----------------|\n",
    "| **NLTK**  | X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n | Tokenization, stemming, POS tagging | B·ªô c√¥ng c·ª• NLP ƒë·∫ßy ƒë·ªß | `pip install nltk` |\n",
    "| **Scikit-Learn**  | Machine Learning | M√¥ h√¨nh ML, TF-IDF | H·ªó tr·ª£ NLP (vector h√≥a vƒÉn b·∫£n) | `pip install scikit-learn` |\n",
    "| **Gensim**  | M√¥ h√¨nh h√≥a ch·ªß ƒë·ªÅ | Word2Vec, LDA, LSI | Hi·ªáu su·∫•t cao | `pip install gensim` |\n",
    "| **TextBlob**  | Ph√¢n t√≠ch vƒÉn b·∫£n | C·∫£m x√∫c, d·ªãch ng√¥n ng·ªØ | D·ªÖ d√πng, d·ª±a tr√™n NLTK | `pip install textblob` |\n",
    "| **SpaCy**  | NLP hi·ªáu su·∫•t cao | Tokenization, parsing | Nhanh, nhi·ªÅu m√¥ h√¨nh pretrained | `pip install spacy` |\n",
    "| **Pattern**  | X·ª≠ l√Ω vƒÉn b·∫£n & web mining | Tr√≠ch xu·∫•t vƒÉn b·∫£n, NLP | H·ªó tr·ª£ c·∫£ x·ª≠ l√Ω web | `pip install pattern` |\n",
    "| **Transformers**  | M√¥ h√¨nh NLP hi·ªán ƒë·∫°i | BERT, GPT, T5 | T√≠ch h·ª£p s·∫µn m√¥ h√¨nh pretrained | `pip install transformers` |\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **C√°c th∆∞ vi·ªán h·ªó tr·ª£ NLP**:  \n",
    "**NLTK, Scikit-Learn, Gensim, TextBlob, SpaCy, Pattern, Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B√ÄI T·∫¨P 2\n",
    "Sinh vi√™n ch·∫°y th·ª±c nghi·ªám c√°c ƒëo·∫°n code sau v√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love NLP. It's fantastic!\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#Code 1\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I love NLP. It's fantastic!\"\n",
    "\n",
    "blob = TextBlob(text)\n",
    "print(blob) # ch·∫°y th·ª≠ coi n√≥ sao thui\n",
    "\n",
    "sentiment = blob.sentiment.polarity # sentiment tr·∫£ v·ªÅ m·∫•y c√°i s·ªë (polarity, subjectivity)\n",
    "\n",
    "print(sentiment)\n",
    "\n",
    "# https://textblob.readthedocs.io/en/dev/quickstart.html\n",
    "\"\"\"\n",
    "Th∆∞ vi·ªán s·ª≠ d·ª•ng trong ƒëo·∫°n code c√≥ ch·ª©c nƒÉng g√¨?\n",
    "    -- The sentiment property returns a namedtuple of the form Sentiment(polarity, subjectivity). \n",
    "    The polarity score is a float within the range [-1.0, 1.0]. \n",
    "    The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
    "    --\n",
    "    Gi·∫£i th√≠ch v·ªÅ Sentiment Analysis trong TextBlob\n",
    "    Polarity (t√≠nh ph√¢n c·ª±c):\n",
    "\n",
    "    Gi√° tr·ªã n·∫±m trong kho·∫£ng [-1.0, 1.0].\n",
    "    -1.0: C·∫£m x√∫c ti√™u c·ª±c.\n",
    "    0.0: Trung l·∫≠p.\n",
    "    1.0: C·∫£m x√∫c t√≠ch c·ª±c.\n",
    "    Subjectivity (t√≠nh ch·ªß quan):\n",
    "\n",
    "    Gi√° tr·ªã n·∫±m trong kho·∫£ng [0.0, 1.0].\n",
    "    0.0: R·∫•t kh√°ch quan (fact-based).\n",
    "    1.0: R·∫•t ch·ªß quan (opinion-based).\n",
    "    --\n",
    "\n",
    "K·∫øt qu·∫£ ch·∫°y th·ª±c nghi·ªám\n",
    "    -- ra k·∫øt qu·∫£ 0.5 \n",
    "Nh·∫≠n x√©t k·∫øt qu·∫£\n",
    "    -- h∆°i h∆°i kh√°ch quan\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpora\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Danh s√°ch c√°c vƒÉn b·∫£n (documents)\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Danh s√°ch c√°c vƒÉn b·∫£n (documents)\n",
    "documents = [\n",
    "    \"Apple and Microsoft are leading technology companies.\",\n",
    "    \"Python and Java are popular programming languages.\",\n",
    "    \"Football and basketball are exciting sports.\"\n",
    "]\n",
    "\n",
    "# Danh s√°ch stopwords (t·ª´ kh√¥ng quan tr·ªçng)\n",
    "stopwords = {\"and\", \"are\"}\n",
    "\"\"\"stopwords ch·ª©a c√°c t·ª´ kh√¥ng c√≥ √Ω nghƒ©a quan tr·ªçng nh∆∞ \"and\", \"are\".\n",
    "texts lo·∫°i b·ªè c√°c stopwords kh·ªèi danh s√°ch t·ª´ trong m·ªói c√¢u\"\"\"\n",
    "# Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: lo·∫°i b·ªè stopwords v√† t√°ch t·ª´\n",
    "texts = [\n",
    "    [word for word in doc.split() if word.lower() not in stopwords] \n",
    "    for doc in documents\n",
    "] #[['Apple', 'Microsoft', 'leading', 'technology', 'companies.'], ['Python', 'Java', 'popular', 'programming', 'languages.'], ['Football', 'basketball', 'exciting', 'sports.']]\n",
    "\n",
    "# T·∫°o t·ª´ ƒëi·ªÉn (m·ªói t·ª´ c√≥ m·ªôt ID duy nh·∫•t)\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\"\"\"Dictionary √°nh x·∫° m·ªói t·ª´ v√†o m·ªôt ID duy nh·∫•t.\n",
    "Corpus chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh d·∫°ng bag-of-words (BOW), m·ªói t·ª´ c√≥ s·ªë l·∫ßn xu·∫•t hi·ªán.\"\"\"\n",
    "# Chuy·ªÉn vƒÉn b·∫£n th√†nh Bag of Words (BoW)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Hu·∫•n luy·ªán m√¥ h√¨nh LDA v·ªõi 3 ch·ªß ƒë·ªÅ (num_topics=3) v√† 15 v√≤ng l·∫∑p (passes=15)\n",
    "lda = gensim.models.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n",
    "\"\"\"LDA (Latent Dirichlet Allocation) ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t√¨m ra c√°c ch·ªß ƒë·ªÅ trong vƒÉn b·∫£n.\n",
    "num_topics=3: Ph√¢n lo·∫°i th√†nh 3 ch·ªß ƒë·ªÅ.\n",
    "passes=15: Ch·∫°y thu·∫≠t to√°n 15 l·∫ßn ƒë·ªÉ c·∫£i thi·ªán k·∫øt qu·∫£.\"\"\"\n",
    "# In ra 3 ch·ªß ƒë·ªÅ, m·ªói ch·ªß ƒë·ªÅ g·ªìm 3 t·ª´ quan tr·ªçng\n",
    "print(lda.print_topics(num_topics=3, num_words=3))\n",
    "\n",
    "\"\"\"\n",
    "Th∆∞ vi·ªán s·ª≠ d·ª•ng trong ƒëo·∫°n code c√≥ ch·ª©c nƒÉng g√¨?\n",
    "    gensim: M·ªôt th∆∞ vi·ªán x·ª≠ l√Ω NLP m·∫°nh m·∫Ω.\n",
    "    corpora: D√πng ƒë·ªÉ t·∫°o t·ª´ ƒëi·ªÉn (dictionary) t·ª´ t·∫≠p d·ªØ li·ªáu vƒÉn b·∫£n.\n",
    "\n",
    "K·∫øt qu·∫£ ch·∫°y th·ª±c nghi·ªám\n",
    "    [(0, '0.098*\"technology\" + 0.098*\"Microsoft\" + 0.098*\"leading\"'), (1, '0.072*\"basketball\" + 0.072*\"Football\" + 0.072*\"exciting\"'), (2, '0.138*\"Java\" + 0.138*\"Python\" + 0.138*\"programming\"')]\n",
    "    M√¥ h√¨nh LDA ƒë√£ ph√¢n lo·∫°i ch·ªß ƒë·ªÅ kh√° ch√≠nh x√°c, d√π ch·ªâ c√≥ 3 c√¢u vƒÉn b·∫£n.\n",
    "\n",
    "Nh·∫≠n x√©t k·∫øt qu·∫£\n",
    "    nh·∫≠n ƒë∆∞·ª£c 3 ph√¢n lo·∫°i th·∫•y c≈©ng kh√° ƒë√∫ng n·∫øu date nhi·ªÅu h∆°n th√¨ ok h∆°n\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 3\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # M√¥ h√¨nh n√†y gi√∫p ph√¢n t√≠ch ng·ªØ ph√°p, g√°n nh√£n t·ª´ lo·∫°i, ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa c√°c t·ª´ trong c√¢u\n",
    "\n",
    "sentence = \"The cat chased the mouse\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "\n",
    "print(token.text, \"-->\", token.dep_)\n",
    "\n",
    "\"\"\"\n",
    "Th∆∞ vi·ªán s·ª≠ d·ª•ng trong ƒëo·∫°n code c√≥ ch·ª©c nƒÉng g√¨?\n",
    "    Th∆∞ vi·ªán x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP) m·∫°nh m·∫Ω, gi√∫p ph√¢n t√≠ch c√∫ ph√°p c√¢u, nh·∫≠n di·ªán th·ª±c th·ªÉ, v√† g√°n nh√£n t·ª´ lo·∫°i.\n",
    "K·∫øt qu·∫£ ch·∫°y th·ª±c nghi·ªám\n",
    "    det: Determiner (m·∫°o t·ª´: \"The\").\n",
    "    nsubj: Nominal Subject (ch·ªß ng·ªØ: \"cat\").\n",
    "    ROOT: T·ª´ g·ªëc c·ªßa c√¢u (ƒë·ªông t·ª´ ch√≠nh: \"chased\").\n",
    "    dobj: Direct Object (t√¢n ng·ªØ tr·ª±c ti·∫øp: \"mouse\").\n",
    "Nh·∫≠n x√©t k·∫øt qu·∫£\n",
    "    K·∫øt qu·∫£ ƒë√∫ng v·ªõi ng·ªØ ph√°p ti·∫øng Anh.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B√ÄI T·∫¨P 3\n",
    "Sinh vi√™n t√¨m hi·ªÉu c√°c ngu·ªìn dataset n√™u tr√™n v√† cho bi·∫øt:\n",
    "\n",
    "- ƒê·∫∑c ƒëi·ªÉm c·ªßa t·ª´ng ngu·ªìn.\n",
    "\n",
    "- B·ªô d·ªØ li·ªáu c√¥ng khai tr√™n c√°c ngu·ªìn thu·ªôc lƒ©nh v·ª±c g√¨ (n√™u r√µ t·ª´ng ngu·ªìn).\n",
    "\n",
    "- D·∫°ng d·ªØ li·ªáu c√¥ng khai.\n",
    "\n",
    "- ∆Øu ƒëi·ªÉm, h·∫°n ch·∫ø (n·∫øu c√≥) c·ªßa t·ª´ng ngu·ªìn.\n",
    "\n",
    "- Ngu·ªìn n√†o cung c·∫•p d·ªØ li·ªáu cho lƒ©nh v·ª±c NLP.\n",
    "\n",
    "- Ch·ªçn 1 b·ªô d·ªØ li·ªáu ƒë·ªÉ t√¨m hi·ªÉu v√† cho bi·∫øt c√°c th√¥ng tin c∆° b·∫£n v·ªÅ b·ªô d·ªØ li·ªáu (vd: lƒ©nh v·ª±c, s·ªë m·∫´u, k√≠ch th∆∞·ªõc, thu·ªôc t√≠nh, ...)\n",
    "\n",
    "- N√™u 01 ngu·ªìn cung c·∫•p d·ªØ li·ªáu NLP uy t√≠n (kh√°c v·ªõi c√°c ngu·ªìn n√™u tr√™n).\n",
    "\n",
    "Sinh vi√™n nh·∫≠p c√¢u tr·∫£ l·ªùi t·∫°i ƒë√¢y\n",
    "\n",
    "D∆∞·ªõi ƒë√¢y l√† h∆∞·ªõng d·∫´n chi ti·∫øt ƒë·ªÉ em th·ª±c hi·ªán b√†i t·∫≠p 3 m·ªôt c√°ch ƒë·∫ßy ƒë·ªß v√† khoa h·ªçc:\n",
    "\n",
    "## 1. ƒê·∫∑c ƒëi·ªÉm c·ªßa t·ª´ng ngu·ªìn dataset\n",
    "\n",
    "Kaggle\tC·ªông ƒë·ªìng chia s·∫ª d·ªØ li·ªáu, b√†i to√°n th·ª±c t·∫ø v√† notebook code m·∫´u.\n",
    "\n",
    "Papers with Code\tCh·ª©a dataset li√™n k·∫øt v·ªõi c√°c nghi√™n c·ª©u khoa h·ªçc, ch·ªß y·∫øu v·ªÅ AI & ML.\n",
    "\n",
    "UCI Machine Learning Repository\tKho d·ªØ li·ªáu h·ªçc m√°y c·ªï ƒëi·ªÉn, d·ªÖ truy c·∫≠p, ƒëa d·∫°ng lƒ©nh v·ª±c.\n",
    "\n",
    "AWS Open Data\tD·ªØ li·ªáu l·ªõn, d√πng cho cloud computing v√† AI.\n",
    "\n",
    "Google Dataset Search\tC√¥ng c·ª• t√¨m ki·∫øm t·ªïng h·ª£p nhi·ªÅu ngu·ªìn dataset.\n",
    "\n",
    "Microsoft Datasets\tD·ªØ li·ªáu t·ª´ Microsoft v·ªÅ AI, th·ªã gi√°c m√°y t√≠nh, NLP.\n",
    "\n",
    "Reddit Datasets\tC·ªông ƒë·ªìng chia s·∫ª nhi·ªÅu dataset ng·∫´u nhi√™n t·ª´ ng∆∞·ªùi d√πng.\n",
    "\n",
    "CMU Libraries\tKho d·ªØ li·ªáu h·ªçc thu·∫≠t t·ª´ Carnegie Mellon University.\n",
    "\n",
    "GitHub Public Datasets\tDanh s√°ch dataset t·ª´ nhi·ªÅu ngu·ªìn tr√™n GitHub.\n",
    "\n",
    "YouTube 8M\tB·ªô d·ªØ li·ªáu video l·ªõn, h·ªó tr·ª£ AI & th·ªã gi√°c m√°y t√≠nh.\n",
    "\n",
    "OpenML\tKho d·ªØ li·ªáu h·ªçc m√°y m·ªü, h·ªó tr·ª£ tr·ª±c ti·∫øp cho ML framework.\n",
    "________________________________________\n",
    "## 2. B·ªô d·ªØ li·ªáu c√¥ng khai tr√™n c√°c ngu·ªìn\n",
    "‚Ä¢\tKaggle: H√¨nh ·∫£nh, vƒÉn b·∫£n, s·ªë li·ªáu th·ªëng k√™, t√†i ch√≠nh, NLP, AI.\n",
    "\n",
    "‚Ä¢\tPapers with Code: Ch·ªß y·∫øu v·ªÅ AI, h·ªçc m√°y, NLP, th·ªã gi√°c m√°y t√≠nh.\n",
    "\n",
    "‚Ä¢\tUCI Machine Learning: D·ªØ li·ªáu c·ªï ƒëi·ªÉn v·ªÅ y t·∫ø, t√†i ch√≠nh, khoa h·ªçc x√£ h·ªôi.\n",
    "\n",
    "‚Ä¢\tAWS Open Data: D·ªØ li·ªáu v·ªá tinh, y t·∫ø, AI, NLP.\n",
    "\n",
    "‚Ä¢\tGoogle Dataset Search: T·ªïng h·ª£p m·ªçi lo·∫°i d·ªØ li·ªáu t·ª´ nhi·ªÅu ngu·ªìn.\n",
    "\n",
    "‚Ä¢\tMicrosoft Datasets: AI, NLP, d·ªØ li·ªáu th·ªùi ti·∫øt, y t·∫ø.\n",
    "\n",
    "‚Ä¢\tReddit Datasets: D·ªØ li·ªáu x√£ h·ªôi, m·∫°ng x√£ h·ªôi, NLP.\n",
    "\n",
    "‚Ä¢\tCMU Libraries: D·ªØ li·ªáu nghi√™n c·ª©u khoa h·ªçc.\n",
    "\n",
    "‚Ä¢\tGitHub Public Datasets: T·ªïng h·ª£p nhi·ªÅu lo·∫°i d·ªØ li·ªáu t·ª´ c·ªông ƒë·ªìng.\n",
    "\n",
    "‚Ä¢\tYouTube 8M: Video, AI, th·ªã gi√°c m√°y t√≠nh.\n",
    "\n",
    "‚Ä¢\tOpenML: D·ªØ li·ªáu h·ªçc m√°y m·ªü, h·ªó tr·ª£ AI & NLP.\n",
    "_______________________________________\n",
    "## 3. D·∫°ng d·ªØ li·ªáu c√¥ng khai\n",
    "‚Ä¢\tCSV, JSON, XML: Kaggle, UCI, Google Dataset Search, OpenML.\n",
    "\n",
    "‚Ä¢\tD·ªØ li·ªáu vƒÉn b·∫£n (TXT, XML, JSON): Papers with Code, AWS Open Data, Microsoft, Reddit.\n",
    "\n",
    "‚Ä¢\tD·ªØ li·ªáu h√¨nh ·∫£nh & video (JPG, PNG, MP4): YouTube 8M, Kaggle, Microsoft.\n",
    "\n",
    "‚Ä¢\tD·ªØ li·ªáu chu·ªói th·ªùi gian (Time-series): AWS, Microsoft, OpenML.\n",
    "________________________________________\n",
    "## 4. ∆Øu ƒëi·ªÉm & H·∫°n ch·∫ø\n",
    "Kaggle\tD·ªØ li·ªáu phong ph√∫, c√≥ notebook m·∫´u\tM·ªôt s·ªë dataset y√™u c·∫ßu ƒëƒÉng nh·∫≠p ƒë·ªÉ t·∫£i v·ªÅ\n",
    "\n",
    "Papers with Code\tD·ªØ li·ªáu k√®m nghi√™n c·ª©u chi ti·∫øt\tCh·ªâ t·∫≠p trung v√†o AI & ML\n",
    "\n",
    "UCI Machine Learning\tD·ªØ li·ªáu ƒë∆°n gi·∫£n, d·ªÖ truy c·∫≠p\tKh√¥ng c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n\n",
    "\n",
    "AWS Open Data\tD·ªØ li·ªáu l·ªõn, h·ªó tr·ª£ AI m·∫°nh m·∫Ω\tC·∫ßn AWS ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn\n",
    "\n",
    "Google Dataset Search\tT√¨m ki·∫øm ƒë∆∞·ª£c nhi·ªÅu ngu·ªìn d·ªØ li·ªáu\tCh·∫•t l∆∞·ª£ng d·ªØ li·ªáu kh√¥ng ƒë·ªìng ƒë·ªÅu\n",
    "\n",
    "Microsoft Datasets\tD·ªØ li·ªáu AI, NLP ch·∫•t l∆∞·ª£ng cao\tKh√¥ng phong ph√∫ b·∫±ng Kaggle\n",
    "\n",
    "Reddit Datasets\tNhi·ªÅu d·ªØ li·ªáu t·ª´ ng∆∞·ªùi d√πng\tKh√¥ng c√≥ ki·ªÉm duy·ªát ch·∫•t l∆∞·ª£ng\n",
    "\n",
    "CMU Libraries\tNgu·ªìn h·ªçc thu·∫≠t ch·∫•t l∆∞·ª£ng\tKh√¥ng c√≥ nhi·ªÅu d·ªØ li·ªáu th·ª±c t·∫ø\n",
    "\n",
    "GitHub Public Datasets\tT·ªïng h·ª£p nhi·ªÅu ngu·ªìn m·ªü\tC·∫ßn ki·ªÉm tra ch·∫•t l∆∞·ª£ng t·ª´ng dataset\n",
    "\n",
    "YouTube 8M\tL·ªõn, h·ªó tr·ª£ nghi√™n c·ª©u video AI\tC·∫ßn x·ª≠ l√Ω ph·ª©c t·∫°p do dung l∆∞·ª£ng l·ªõn\n",
    "\n",
    "OpenML\tH·ªó tr·ª£ tr·ª±c ti·∫øp cho h·ªçc m√°y\tD·ªØ li·ªáu kh√¥ng ƒëa d·∫°ng b·∫±ng Kaggle\n",
    "________________________________________\n",
    "## 5. Ngu·ªìn cung c·∫•p d·ªØ li·ªáu cho NLP\n",
    "‚Ä¢\tKaggle\n",
    "\n",
    "‚Ä¢\tPapers with Code\n",
    "\n",
    "‚Ä¢\tAWS Open Data\n",
    "\n",
    "‚Ä¢\tMicrosoft Datasets\n",
    "\n",
    "‚Ä¢\tReddit Datasets\n",
    "\n",
    "‚Ä¢\tOpenML\n",
    "________________________________________\n",
    "## 6. Ch·ªçn 1 b·ªô d·ªØ li·ªáu ƒë·ªÉ t√¨m hi·ªÉu\n",
    "‚Ä¢\tLƒ©nh v·ª±c: X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP) - ph√¢n t√≠ch c·∫£m x√∫c\n",
    "\n",
    "‚Ä¢\tS·ªë m·∫´u: 1.6 tri·ªáu tweet\n",
    "\n",
    "‚Ä¢\tK√≠ch th∆∞·ªõc: ~200 MB\n",
    "\n",
    "‚Ä¢\tThu·ªôc t√≠nh: \n",
    "\n",
    "o\ttarget (c·∫£m x√∫c: 0 = ti√™u c·ª±c, 4 = t√≠ch c·ª±c)\n",
    "\n",
    "o\ttext (n·ªôi dung tweet)\n",
    "\n",
    "o\tdate, user, query (c√°c th√¥ng tin b·ªï sung)\n",
    "\n",
    "‚Ä¢\t·ª®ng d·ª•ng: D√πng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n t√≠ch c·∫£m x√∫c trong NLP.\n",
    "________________________________________\n",
    "## 7. Ngu·ªìn cung c·∫•p d·ªØ li·ªáu NLP uy t√≠n kh√°c\n",
    "‚Ä¢\tHugging Face Datasets ‚Äì Cung c·∫•p nhi·ªÅu d·ªØ li·ªáu NLP ch·∫•t l∆∞·ª£ng cao, t√≠ch h·ª£p v·ªõi th∆∞ vi·ªán Transformers ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh AI. üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C√°c ngu·ªìn Dataset\n",
    "- Kaggle: https://www.kaggle.com/\n",
    "\n",
    "- Papers with Code: https://paperswithcode.com/datasets\n",
    "\n",
    "- UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/index.php\n",
    "\n",
    "- Registry of Open Data on AWS: https://registry.opendata.aws\n",
    "\n",
    "- Google Dataset Search: https://datasetsearch.research.google.com/\n",
    "\n",
    "- Microsoft Datasets: https://msropendata.com/\n",
    "\n",
    "- Reddit datasets: https://www.reddit.com/r/datasets/top/?sort=top&t=all\n",
    "\n",
    "- CMU Libraries: https://guides.library.cmu.edu/az.php\n",
    "\n",
    "- Public Datasets tr√™n Github: https://github.com/awesomedata/awesome-public-datasets#machinelearning\n",
    "\n",
    "- YouTube Dataset: https://research.google.com/youtube8m/\n",
    "\n",
    "- OpenML: https://www.openml.org/search?type=data&sort=runs&status=active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
